import streamlit as st

# ğŸš¨ é‡è¦ï¼šst.set_page_config å¿…é ˆæ˜¯ç¬¬ä¸€å€‹ Streamlit å‘½ä»¤
st.set_page_config(page_title="åœ‹æ³°äººå£½ - ç”¨æˆ¶è¡Œç‚ºé æ¸¬å·¥å…·", layout="centered", initial_sidebar_state="collapsed")

import pandas as pd
import numpy as np
from datetime import datetime
import plotly.express as px

# å˜—è©¦è¼‰å…¥ TensorFlow å’Œç›¸é—œå¥—ä»¶
try:
    from tensorflow.keras.models import load_model
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from joblib import load
    TF_AVAILABLE = True
except ImportError as e:
    st.error(f"è¼‰å…¥ä¾è³´å¥—ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    TF_AVAILABLE = False

# åˆå§‹åŒ– session state
if "prediction_data" not in st.session_state:
    st.session_state.prediction_data = None
if "all_columns" not in st.session_state:
    st.session_state.all_columns = []

# æ¨¡å‹èˆ‡ç·¨ç¢¼å™¨è¼‰å…¥
SEQ_LEN = 10
cat_features = ['action_group', 'source', 'medium', 'platform']
num_features = ['staytime', 'has_shared', 'revisit_count']

@st.cache_resource
def load_model_and_encoders():
   if not TF_AVAILABLE:
       st.error("TensorFlow æœªæ­£ç¢ºå®‰è£ï¼Œç„¡æ³•è¼‰å…¥æ¨¡å‹")
       return None, None, None
   
   # è¨ºæ–·æª”æ¡ˆç‹€æ…‹
   import os
   model_file = "lstm_multi_output_model_v2.h5"
   
   if os.path.exists(model_file):
       file_size = os.path.getsize(model_file)
       st.info(f"æ¨¡å‹æª”æ¡ˆå­˜åœ¨ï¼Œå¤§å°: {file_size} bytes ({file_size/1024/1024:.2f} MB)")
   
   try:
       # å…ˆè¼‰å…¥æ¨¡å‹
       st.info("æ­£åœ¨è¼‰å…¥ LSTM æ¨¡å‹...")
       model = load_model("lstm_multi_output_model_v2.h5")
       st.success("âœ… LSTM æ¨¡å‹è¼‰å…¥æˆåŠŸ")
       
       # é€ä¸€è¼‰å…¥ encoders
       encoders = {}
       for col in cat_features:
           encoder_file = f'encoder_{col}.pkl'
           try:
               if os.path.exists(encoder_file):
                   file_size = os.path.getsize(encoder_file)
                   st.info(f"è¼‰å…¥ {encoder_file} (å¤§å°: {file_size} bytes)")
                   
                   if file_size < 500:  # æª”æ¡ˆå¤ªå°å¯èƒ½æœ‰å•é¡Œ
                       st.warning(f"âš ï¸ {encoder_file} æª”æ¡ˆå¤§å°ç•°å¸¸: {file_size} bytes")
                       
                       # æª¢æŸ¥æ˜¯å¦ç‚º LFS æŒ‡é‡æª”æ¡ˆ
                       try:
                           with open(encoder_file, 'r', encoding='utf-8') as f:
                               content = f.read()
                               st.error(f"æª”æ¡ˆå…§å®¹ï¼ˆLFS æŒ‡é‡ï¼‰: {content}")
                               return None, None, None
                       except:
                           with open(encoder_file, 'rb') as f:
                               content = f.read()
                               st.error(f"æª”æ¡ˆå…§å®¹ï¼ˆäºŒé€²åˆ¶ï¼‰: {content}")
                               return None, None, None
                   
                   encoder = load(encoder_file)
                   encoders[col] = encoder
                   st.success(f"âœ… {col} encoder è¼‰å…¥æˆåŠŸï¼Œé¡åˆ¥æ•¸: {len(encoder.classes_)}")
               else:
                   st.error(f"âŒ æ‰¾ä¸åˆ° {encoder_file}")
                   return None, None, None
           except Exception as e:
               st.error(f"âŒ è¼‰å…¥ {encoder_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
               return None, None, None
       
       # é€ä¸€è¼‰å…¥ scalers
       scalers = {}
       for col in num_features:
           scaler_file = f'scaler_feature_{col}.pkl'
           try:
               if os.path.exists(scaler_file):
                   file_size = os.path.getsize(scaler_file)
                   st.info(f"è¼‰å…¥ {scaler_file} (å¤§å°: {file_size} bytes)")
                   
                   if file_size < 500:  # æª”æ¡ˆå¤ªå°å¯èƒ½æœ‰å•é¡Œ
                       st.warning(f"âš ï¸ {scaler_file} æª”æ¡ˆå¤§å°ç•°å¸¸: {file_size} bytes")
                       
                       # æª¢æŸ¥æ˜¯å¦ç‚º LFS æŒ‡é‡æª”æ¡ˆ
                       try:
                           with open(scaler_file, 'r', encoding='utf-8') as f:
                               content = f.read()
                               st.error(f"æª”æ¡ˆå…§å®¹ï¼ˆLFS æŒ‡é‡ï¼‰: {content}")
                               return None, None, None
                       except:
                           with open(scaler_file, 'rb') as f:
                               content = f.read()
                               st.error(f"æª”æ¡ˆå…§å®¹ï¼ˆäºŒé€²åˆ¶ï¼‰: {content}")
                               return None, None, None
                   
                   scaler = load(scaler_file)
                   scalers[col] = scaler
                   st.success(f"âœ… {col} scaler è¼‰å…¥æˆåŠŸ")
               else:
                   st.error(f"âŒ æ‰¾ä¸åˆ° {scaler_file}")
                   return None, None, None
           except Exception as e:
               st.error(f"âŒ è¼‰å…¥ {scaler_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
               return None, None, None
               
       return model, encoders, scalers
       
   except Exception as e:
       st.error(f"è¼‰å…¥éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
       return None, None, None

def safe_label_transform(encoder, value, default=-1):
    try:
        return encoder.transform([value])[0]
    except:
        return default

def safe_scale(scaler, value, default=0.0):
    try:
        return scaler.transform(np.array([[value]])).flatten()[0]
    except:
        return default

def recommend_strategy(action_group):
    if "è©¦ç®—" in action_group:
        return "æ¨æ’­å„ªæƒ åˆ¸"
    elif "å•†å“è³‡è¨Šé " in action_group or "è¨‚è£½ä¿éšªçµ„åˆ" in action_group:
        return "å»ºè­°å±•ç¤ºå…§å®¹"
    elif "é ç´„" in action_group:
        return "æä¾›é ç´„"
    else:
        return "ç­‰å¾…æ›´å¤šè³‡æ–™"



def safe_label_transform(encoder, value, default=0):  # æ”¹ç‚º default=0
    """
    å®‰å…¨åœ°é€²è¡Œ label encodingï¼Œé¿å…æœªçŸ¥å€¼å°è‡´éŒ¯èª¤
    """
    try:
        # ç¢ºä¿å€¼ä¸æ˜¯ NaN æˆ– None
        if pd.isna(value) or value is None:
            return default
        
        # è½‰æ›ç‚ºå­—ç¬¦ä¸²ï¼ˆå› ç‚ºè¨“ç·´æ™‚ä½¿ç”¨äº† astype(str)ï¼‰
        str_value = str(value)
        
        # æª¢æŸ¥æ˜¯å¦åœ¨å·²çŸ¥é¡åˆ¥ä¸­
        if str_value in encoder.classes_:
            return encoder.transform([str_value])[0]
        else:
            # æœªçŸ¥é¡åˆ¥è¿”å› 0ï¼ˆå¡«å……å€¼ï¼‰
            print(f"è­¦å‘Šï¼šæœªçŸ¥é¡åˆ¥ '{str_value}'ï¼Œä½¿ç”¨é»˜èªå€¼ {default}")
            return default
    except Exception as e:
        print(f"Label encoding éŒ¯èª¤: {e}ï¼Œä½¿ç”¨é»˜èªå€¼ {default}")
        return default

def safe_scale(scaler, value, default=0.0):
    """
    å®‰å…¨åœ°é€²è¡Œæ•¸å€¼æ¨™æº–åŒ–
    """
    try:
        # ç¢ºä¿å€¼ä¸æ˜¯ NaN æˆ– None
        if pd.isna(value) or value is None:
            return default
        
        # è½‰æ›ç‚ºæµ®é»æ•¸
        float_value = float(value)
        
        # é€²è¡Œæ¨™æº–åŒ–
        scaled_value = scaler.transform([[float_value]])[0][0]
        
        # æª¢æŸ¥çµæœæ˜¯å¦æœ‰æ•ˆ
        if pd.isna(scaled_value):
            return default
        
        return scaled_value
    except Exception as e:
        print(f"Scaling éŒ¯èª¤: {e}ï¼Œä½¿ç”¨é»˜èªå€¼ {default}")
        return default

def create_unknown_category_mapping(encoders):
    """
    ç‚ºæ¯å€‹ encoder å‰µå»ºæœªçŸ¥é¡åˆ¥çš„æ˜ å°„ç´¢å¼•
    """
    mapping = {}
    for col, encoder in encoders.items():
        # ä½¿ç”¨é¡åˆ¥æ•¸é‡ä½œç‚ºæœªçŸ¥é¡åˆ¥çš„ç´¢å¼•ï¼ˆç¢ºä¿ä¸è¶…å‡º embedding ç¯„åœï¼‰
        unknown_index = min(len(encoder.classes_), encoder.transform(encoder.classes_).max())
        mapping[col] = unknown_index
    return mapping

def safe_label_transform(encoder, value, default=0):
    """
    å®‰å…¨åœ°é€²è¡Œ label encodingï¼Œç¢ºä¿ä¸æœƒç”¢ç”Ÿè² å€¼
    """
    try:
        # è™•ç† NaN, None, ç©ºå­—ç¬¦ä¸²
        if pd.isna(value) or value is None or value == '' or value == 'nan':
            return default
        
        # è½‰æ›ç‚ºå­—ç¬¦ä¸²ï¼ˆä¿æŒèˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼‰
        str_value = str(value).strip()
        
        # å¦‚æœæ˜¯ç©ºå­—ç¬¦ä¸²ï¼Œè¿”å›é»˜èªå€¼
        if str_value == '' or str_value == 'nan':
            return default
        
        # æª¢æŸ¥æ˜¯å¦åœ¨å·²çŸ¥é¡åˆ¥ä¸­
        if str_value in encoder.classes_:
            result = encoder.transform([str_value])[0]
            # ç¢ºä¿çµæœä¸æ˜¯è² æ•¸
            if result < 0:
                print(f"è­¦å‘Šï¼šç·¨ç¢¼çµæœç‚ºè² æ•¸ {result}ï¼Œä½¿ç”¨é»˜èªå€¼ {default}")
                return default
            return result
        else:
            # æœªçŸ¥é¡åˆ¥ï¼Œè¿”å›é»˜èªå€¼
            print(f"è­¦å‘Šï¼šæœªçŸ¥é¡åˆ¥ '{str_value}' (åŸå€¼: {value})ï¼Œä½¿ç”¨é»˜èªå€¼ {default}")
            return default
            
    except Exception as e:
        print(f"Label encoding éŒ¯èª¤ (å€¼: {value}): {e}ï¼Œä½¿ç”¨é»˜èªå€¼ {default}")
        return default

def safe_scale(scaler, value, default=0.0):
    """
    å®‰å…¨åœ°é€²è¡Œæ•¸å€¼æ¨™æº–åŒ–
    """
    try:
        # è™•ç† NaN, None
        if pd.isna(value) or value is None:
            return default
        
        # è½‰æ›ç‚ºæµ®é»æ•¸
        float_value = float(value)
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰é™æ•¸
        if not np.isfinite(float_value):
            return default
        
        # é€²è¡Œæ¨™æº–åŒ–
        scaled_value = scaler.transform([[float_value]])[0][0]
        
        # æª¢æŸ¥çµæœæ˜¯å¦æœ‰æ•ˆ
        if not np.isfinite(scaled_value):
            return default
        
        return scaled_value
        
    except Exception as e:
        print(f"Scaling éŒ¯èª¤ (å€¼: {value}): {e}ï¼Œä½¿ç”¨é»˜èªå€¼ {default}")
        return default

def validate_and_fix_sequence(seq, seq_name):
    """
    é©—è­‰ä¸¦ä¿®å¾©åºåˆ—ä¸­çš„ç„¡æ•ˆå€¼
    """
    seq = np.array(seq)
    
    # æª¢æŸ¥ä¸¦ä¿®å¾©è² å€¼
    negative_mask = seq < 0
    if np.any(negative_mask):
        print(f"è­¦å‘Šï¼š{seq_name} ä¸­ç™¼ç¾ {np.sum(negative_mask)} å€‹è² å€¼ï¼Œå·²ä¿®å¾©ç‚º 0")
        seq[negative_mask] = 0
    
    # æª¢æŸ¥ä¸¦ä¿®å¾© NaN å€¼
    nan_mask = ~np.isfinite(seq)
    if np.any(nan_mask):
        print(f"è­¦å‘Šï¼š{seq_name} ä¸­ç™¼ç¾ {np.sum(nan_mask)} å€‹ç„¡æ•ˆå€¼ï¼Œå·²ä¿®å¾©ç‚º 0")
        seq[nan_mask] = 0
    
    return seq

def predict_from_uploaded_csv(df):
    if model is None or encoders is None or scalers is None:
        st.error("æ¨¡å‹æœªæ­£ç¢ºè¼‰å…¥ï¼Œç„¡æ³•é€²è¡Œé æ¸¬")
        return pd.DataFrame()
    
    # æª¢æŸ¥å¿…è¦æ¬„ä½
    required_columns = ['user_pseudo_id', 'event_time'] + cat_features + num_features
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        st.error(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
        return pd.DataFrame()
    
    # è³‡æ–™é è™•ç†
    df = df.copy()
    df = df.sort_values(by=["user_pseudo_id", "event_time"])
    
    # åˆ—å°ç·¨ç¢¼å™¨ä¿¡æ¯ç”¨æ–¼é™¤éŒ¯
    print("\n=== ç·¨ç¢¼å™¨ä¿¡æ¯ ===")
    for col, encoder in encoders.items():
        print(f"{col}: {len(encoder.classes_)} å€‹é¡åˆ¥, ç¯„åœ [0, {len(encoder.classes_)-1}]")
        print(f"  å‰5å€‹é¡åˆ¥: {list(encoder.classes_[:5])}")
    
    results = []
    
    for user_id, group in df.groupby("user_pseudo_id"):
        try:
            print(f"\nè™•ç†ç”¨æˆ¶: {user_id}")
            last_steps = group.tail(SEQ_LEN)
            pad_len = SEQ_LEN - len(last_steps)
            print(f"  åºåˆ—é•·åº¦: {len(last_steps)}, éœ€è¦å¡«å……: {pad_len}")

            # è™•ç†é¡åˆ¥ç‰¹å¾µ (4å€‹è¼¸å…¥)
            cat_inputs = []
            for i, col in enumerate(cat_features):
                print(f"  è™•ç†é¡åˆ¥ç‰¹å¾µ: {col}")
                
                # æª¢æŸ¥åŸå§‹æ•¸æ“š
                raw_values = last_steps[col].tolist()
                print(f"    åŸå§‹å€¼: {raw_values}")
                
                # é€²è¡Œç·¨ç¢¼
                seq = []
                for v in raw_values:
                    encoded_value = safe_label_transform(encoders[col], v, default=0)
                    seq.append(encoded_value)
                
                print(f"    ç·¨ç¢¼å¾Œ: {seq}")
                
                # å¡«å……åºåˆ—
                padded_seq = [0] * pad_len + seq
                print(f"    å¡«å……å¾Œ: {padded_seq}")
                
                # é©—è­‰ä¸¦ä¿®å¾©åºåˆ—
                final_seq = validate_and_fix_sequence(padded_seq, f"{col}_sequence")
                
                # ç¢ºä¿æ•¸æ“šé¡å‹å’Œå½¢ç‹€
                final_seq = np.array(final_seq, dtype=np.int32).reshape(1, SEQ_LEN)
                
                # æœ€çµ‚æª¢æŸ¥
                if final_seq.min() < 0:
                    st.error(f"æœ€çµ‚åºåˆ—ä»åŒ…å«è² å€¼: {final_seq}")
                    return pd.DataFrame()
                
                if final_seq.max() >= len(encoders[col].classes_):
                    st.error(f"åºåˆ—å€¼è¶…å‡ºç¯„åœ: max={final_seq.max()}, å…è¨±ç¯„åœ=[0, {len(encoders[col].classes_)-1}]")
                    return pd.DataFrame()
                
                cat_inputs.append(final_seq)
                print(f"    æœ€çµ‚å½¢ç‹€: {final_seq.shape}, ç¯„åœ: [{final_seq.min()}, {final_seq.max()}]")

            # è™•ç†æ•¸å€¼ç‰¹å¾µ (3å€‹è¼¸å…¥)
            num_inputs = []
            for col in num_features:
                print(f"  è™•ç†æ•¸å€¼ç‰¹å¾µ: {col}")
                
                # æª¢æŸ¥åŸå§‹æ•¸æ“š
                raw_values = last_steps[col].tolist()
                print(f"    åŸå§‹å€¼: {raw_values}")
                
                # é€²è¡Œæ¨™æº–åŒ–
                seq = []
                for v in raw_values:
                    scaled_value = safe_scale(scalers[col], v, default=0.0)
                    seq.append(scaled_value)
                
                print(f"    æ¨™æº–åŒ–å¾Œ: {seq}")
                
                # å¡«å……åºåˆ—
                padded_seq = [0.0] * pad_len + seq
                
                # é©—è­‰ä¸¦ä¿®å¾©åºåˆ—
                final_seq = validate_and_fix_sequence(padded_seq, f"{col}_sequence")
                
                # ç¢ºä¿æ•¸æ“šé¡å‹å’Œå½¢ç‹€
                final_seq = np.array(final_seq, dtype=np.float32).reshape(1, SEQ_LEN)
                num_inputs.append(final_seq)
                print(f"    æœ€çµ‚å½¢ç‹€: {final_seq.shape}")

            # çµ„åˆæ‰€æœ‰è¼¸å…¥
            all_inputs = cat_inputs + num_inputs
            
            print(f"  ç¸½è¼¸å…¥æ•¸é‡: {len(all_inputs)} (æœŸæœ›: 7)")
            
            # æœ€çµ‚é©—è­‰
            for i, inp in enumerate(all_inputs):
                input_name = cat_features[i] if i < len(cat_features) else num_features[i - len(cat_features)]
                print(f"    è¼¸å…¥ {i+1} ({input_name}): å½¢ç‹€={inp.shape}, é¡å‹={inp.dtype}")
                
                if i < len(cat_features):  # é¡åˆ¥ç‰¹å¾µ
                    if inp.min() < 0:
                        st.error(f"é¡åˆ¥è¼¸å…¥ {input_name} åŒ…å«è² å€¼: {inp.min()}")
                        return pd.DataFrame()
                    
                    max_allowed = len(encoders[cat_features[i]].classes_) - 1
                    if inp.max() > max_allowed:
                        st.error(f"é¡åˆ¥è¼¸å…¥ {input_name} å€¼è¶…å‡ºç¯„åœ: {inp.max()} > {max_allowed}")
                        return pd.DataFrame()

            # é€²è¡Œé æ¸¬
            print("  é–‹å§‹é æ¸¬...")
            predictions = model.predict(all_inputs, verbose=0)
            y_pred_action_group, y_pred_online, y_pred_o2o = predictions
            print("  é æ¸¬å®Œæˆ")

            # è™•ç†é æ¸¬çµæœ
            top5_indices = y_pred_action_group[0].argsort()[-5:][::-1]
            top5_confidences = y_pred_action_group[0][top5_indices]

            inv_action = {i: v for i, v in enumerate(encoders['action_group'].classes_)}
            top5_actions = [inv_action.get(idx, "æœªçŸ¥é¡åˆ¥") for idx in top5_indices]

            strategy = recommend_strategy(top5_actions[0])

            result = {
                "user_pseudo_id": user_id,
                "Top1_next_action_group": top5_actions[0],
                "Top1_confidence": round(float(top5_confidences[0]), 4),
                "Online_conversion_prob": round(float(y_pred_online[0][0]), 4),
                "O2O_reservation_prob": round(float(y_pred_o2o[0][0]), 4),
                "Marketing_Strategy": strategy
            }
            
            # æ·»åŠ  Top 5 çµæœ
            for i in range(5):
                result[f"Top{i+1}_next_action_group"] = top5_actions[i]
                result[f"Top{i+1}_confidence"] = round(float(top5_confidences[i]), 4)

            results.append(result)
            print(f"  ç”¨æˆ¶ {user_id} è™•ç†å®Œæˆ")
            
        except Exception as e:
            st.error(f"è™•ç†ç”¨æˆ¶ {user_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            print(f"éŒ¯èª¤è©³æƒ…: {e}")
            import traceback
            traceback.print_exc()
            continue

    print(f"\nç¸½å…±è™•ç†äº† {len(results)} å€‹ç”¨æˆ¶")
    return pd.DataFrame(results)

# UI ä»‹é¢
st.title("ğŸ“Š åœ‹æ³°äººå£½ â€“ å¤šå…ƒè¨ªå®¢è¡Œç‚ºé æ¸¬å·¥å…·")

# è¼‰å…¥æ¨¡å‹ä¸¦é¡¯ç¤ºç‹€æ…‹
with st.spinner("æ­£åœ¨è¼‰å…¥æ¨¡å‹..."):
    if TF_AVAILABLE:
        model, encoders, scalers = load_model_and_encoders()
        if model is not None:
            st.success("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        else:
            st.error("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—")
            st.stop()
    else:
        st.error("âš ï¸ TensorFlow ä¸å¯ç”¨")
        st.stop()

st.markdown("è«‹ä¸Šå‚³åŒ…å«ç”¨æˆ¶è¡Œç‚ºæ­·ç¨‹è³‡æ–™çš„ CSV é€²è¡Œæ¨¡å‹é æ¸¬")

# æª”æ¡ˆä¸Šå‚³å™¨
uploaded_file = st.file_uploader(
    "ğŸ“¥ ä¸Šå‚³ CSVï¼ˆéœ€å«æ¬„ä½ï¼šuser_pseudo_id, event_time, action_group, source, medium, platform, staytime, has_shared, revisit_countï¼‰", 
    type=["csv"]
)

if uploaded_file is not None:
    try:
        user_df = pd.read_csv(uploaded_file)
        st.success(f"âœ… æˆåŠŸè®€å– {len(user_df)} ç­†è³‡æ–™ï¼Œé–‹å§‹æ¨¡å‹é æ¸¬...")
        st.dataframe(user_df.head(10), use_container_width=True)

        with st.spinner("æ­£åœ¨é€²è¡Œé æ¸¬..."):
            df_result = predict_from_uploaded_csv(user_df)
            
        if not df_result.empty:
            st.session_state.prediction_data = df_result
            st.session_state.all_columns = df_result.columns.tolist()
            st.success("ğŸ‰ é æ¸¬å®Œæˆï¼")
        else:
            st.error("é æ¸¬å¤±æ•—ï¼Œè«‹æª¢æŸ¥è³‡æ–™æ ¼å¼")

    except Exception as e:
        st.error(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")

# é æ¸¬çµæœå±•ç¤ºèˆ‡ä¸‹è¼‰
if st.session_state.prediction_data is not None:
    df = st.session_state.prediction_data
    st.markdown("### ğŸ“‹ æ¨¡å‹é æ¸¬çµæœ")
    st.dataframe(df, use_container_width=True)

    with st.expander("ğŸ”§ é¸æ“‡æ¬„ä½ä¸‹è¼‰çµæœ", expanded=True):
        selected_columns = st.multiselect(
            "è«‹é¸æ“‡è¦è¼¸å‡ºçš„æ¬„ä½ï¼š",
            options=st.session_state.all_columns,
            default=st.session_state.all_columns
        )

    if selected_columns:
        csv = df[selected_columns].to_csv(index=False).encode("utf-8-sig")
        st.download_button("ğŸ“¥ ä¸‹è¼‰çµæœ CSV", data=csv, file_name="prediction_result.csv", mime="text/csv")
    else:
        st.warning("è«‹é¸æ“‡è‡³å°‘ä¸€å€‹æ¬„ä½")

    # è¦–è¦ºåŒ–åœ–è¡¨
    st.markdown("### ğŸ“Š çµ±è¨ˆåœ–è¡¨")

    if len(df) > 0:
        # åœ–è¡¨ 1: Top1 è¡Œç‚ºåˆ†ä½ˆ
        chart_df = df["Top1_next_action_group"].value_counts().reset_index()
        chart_df.columns = ["action_group", "count"]
        fig1 = px.bar(chart_df, x="action_group", y="count", title="Top1 è¡Œç‚ºåˆ†ä½ˆ")
        st.plotly_chart(fig1, use_container_width=True)

        # åœ–è¡¨ 2: ä¿¡å¿ƒåˆ†æ•¸åˆ†ä½ˆ
        fig2 = px.histogram(df, x="Top1_confidence", nbins=20, title="Top1 ä¿¡å¿ƒåˆ†æ•¸åˆ†ä½ˆ")
        st.plotly_chart(fig2, use_container_width=True)

        # åœ–è¡¨ 3: æ•£é»åœ–
        fig3 = px.scatter(df, x="Top1_confidence", y="Online_conversion_prob", title="ä¿¡å¿ƒåˆ†æ•¸ vs ç·šä¸Šè½‰æ›æ©Ÿç‡")
        st.plotly_chart(fig3, use_container_width=True)

        # åœ–è¡¨ 4: è¡ŒéŠ·ç­–ç•¥æ¯”ä¾‹
        strategy_df = df["Marketing_Strategy"].value_counts().reset_index()
        strategy_df.columns = ["strategy", "count"]
        fig4 = px.pie(strategy_df, names="strategy", values="count", title="å»ºè­°è¡ŒéŠ·ç­–ç•¥æ¯”ä¾‹")
        st.plotly_chart(fig4, use_container_width=True)
