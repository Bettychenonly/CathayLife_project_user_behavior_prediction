import streamlit as st

# ğŸš¨ é‡è¦ï¼šst.set_page_config å¿…é ˆæ˜¯ç¬¬ä¸€å€‹ Streamlit å‘½ä»¤
st.set_page_config(page_title="åœ‹æ³°äººå£½ - ç”¨æˆ¶è¡Œç‚ºé æ¸¬å·¥å…·", layout="wide", initial_sidebar_state="collapsed")

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import plotly.express as px

# å˜—è©¦è¼‰å…¥ TensorFlow å’Œç›¸é—œå¥—ä»¶
try:
    from tensorflow.keras.models import load_model
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from joblib import load
    TF_AVAILABLE = True
except ImportError as e:
    st.error(f"è¼‰å…¥ä¾è³´å¥—ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    TF_AVAILABLE = False

# åˆå§‹åŒ– session state
if "prediction_data" not in st.session_state:
    st.session_state.prediction_data = None
if "all_columns" not in st.session_state:
    st.session_state.all_columns = []
if "uploaded_file_name" not in st.session_state:
    st.session_state.uploaded_file_name = None
if "processed_data" not in st.session_state:
    st.session_state.processed_data = None
if "raw_uploaded_data" not in st.session_state:
    st.session_state.raw_uploaded_data = None

# æ¨¡å‹èˆ‡ç·¨ç¢¼å™¨è¼‰å…¥
SEQ_LEN = 10
cat_features = ['action_group', 'source', 'medium', 'platform']
num_features = ['staytime', 'has_shared', 'revisit_count']

@st.cache_resource
def load_model_and_encoders():
   if not TF_AVAILABLE:
       st.error("TensorFlow æœªæ­£ç¢ºå®‰è£ï¼Œç„¡æ³•è¼‰å…¥æ¨¡å‹")
       return None, None, None
   
   # è¨ºæ–·æª”æ¡ˆç‹€æ…‹
   import os
   model_file = "lstm_multi_output_model_v2.h5"
   
   if os.path.exists(model_file):
       file_size = os.path.getsize(model_file)
       st.info(f"æ¨¡å‹æª”æ¡ˆå­˜åœ¨ï¼Œå¤§å°: {file_size} bytes ({file_size/1024/1024:.2f} MB)")
   
   try:
       # å…ˆè¼‰å…¥æ¨¡å‹
       st.info("æ­£åœ¨è¼‰å…¥ LSTM æ¨¡å‹...")
       model = load_model("lstm_multi_output_model_v2.h5")
       st.success("âœ… LSTM æ¨¡å‹è¼‰å…¥æˆåŠŸ")
       
       # é€ä¸€è¼‰å…¥ encoders
       encoders = {}
       for col in cat_features:
           encoder_file = f'encoder_{col}.pkl'
           try:
               if os.path.exists(encoder_file):
                   file_size = os.path.getsize(encoder_file)
                   st.info(f"è¼‰å…¥ {encoder_file} (å¤§å°: {file_size} bytes)")
                   
                   if file_size < 500:  # æª”æ¡ˆå¤ªå°å¯èƒ½æœ‰å•é¡Œ
                       st.warning(f"âš ï¸ {encoder_file} æª”æ¡ˆå¤§å°ç•°å¸¸: {file_size} bytes")
                       
                       # æª¢æŸ¥æ˜¯å¦ç‚º LFS æŒ‡é‡æª”æ¡ˆ
                       try:
                           with open(encoder_file, 'r', encoding='utf-8') as f:
                               content = f.read()
                               st.error(f"æª”æ¡ˆå…§å®¹ï¼ˆLFS æŒ‡é‡ï¼‰: {content}")
                               return None, None, None
                       except:
                           with open(encoder_file, 'rb') as f:
                               content = f.read()
                               st.error(f"æª”æ¡ˆå…§å®¹ï¼ˆäºŒé€²åˆ¶ï¼‰: {content}")
                               return None, None, None
                   
                   encoder = load(encoder_file)
                   encoders[col] = encoder
                   st.success(f"âœ… {col} encoder è¼‰å…¥æˆåŠŸï¼Œé¡åˆ¥æ•¸: {len(encoder.classes_)}")
               else:
                   st.error(f"âŒ æ‰¾ä¸åˆ° {encoder_file}")
                   return None, None, None
           except Exception as e:
               st.error(f"âŒ è¼‰å…¥ {encoder_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
               return None, None, None
       
       # é€ä¸€è¼‰å…¥ scalers
       scalers = {}
       for col in num_features:
           scaler_file = f'scaler_feature_{col}.pkl'
           try:
               if os.path.exists(scaler_file):
                   file_size = os.path.getsize(scaler_file)
                   st.info(f"è¼‰å…¥ {scaler_file} (å¤§å°: {file_size} bytes)")
                   
                   if file_size < 500:  # æª”æ¡ˆå¤ªå°å¯èƒ½æœ‰å•é¡Œ
                       st.warning(f"âš ï¸ {scaler_file} æª”æ¡ˆå¤§å°ç•°å¸¸: {file_size} bytes")
                       
                       # æª¢æŸ¥æ˜¯å¦ç‚º LFS æŒ‡é‡æª”æ¡ˆ
                       try:
                           with open(scaler_file, 'r', encoding='utf-8') as f:
                               content = f.read()
                               st.error(f"æª”æ¡ˆå…§å®¹ï¼ˆLFS æŒ‡é‡ï¼‰: {content}")
                               return None, None, None
                       except:
                           with open(scaler_file, 'rb') as f:
                               content = f.read()
                               st.error(f"æª”æ¡ˆå…§å®¹ï¼ˆäºŒé€²åˆ¶ï¼‰: {content}")
                               return None, None, None
                   
                   scaler = load(scaler_file)
                   scalers[col] = scaler
                   st.success(f"âœ… {col} scaler è¼‰å…¥æˆåŠŸ")
               else:
                   st.error(f"âŒ æ‰¾ä¸åˆ° {scaler_file}")
                   return None, None, None
           except Exception as e:
               st.error(f"âŒ è¼‰å…¥ {scaler_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
               return None, None, None
               
       return model, encoders, scalers
       
   except Exception as e:
       st.error(f"è¼‰å…¥éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
       return None, None, None

# 1. å®šç¾©ç­‰ç´šæ˜ å°„å‡½å¼
def get_level(action_group: str) -> float:
    # æª¢æŸ¥è¼¸å…¥æ˜¯å¦ç‚ºæœ‰æ•ˆå€¼
    if action_group is None or pd.isna(action_group) or action_group == '':
        return -1
        
    # è½‰æ›ç‚ºå­—ç¬¦ä¸²ä»¥ç¢ºä¿ä¸€è‡´æ€§
    action_group = str(action_group).strip()
    
    # ç­‰ç´š 0
    if action_group in {"å…¶ä»–", "ä¿éšªè¦–åœ–ã€ä¿å–®æ˜ç´°ã€è³‡ç”¢ç¸½è¦½ã€ä¿éšªæ˜ç´°"}:
        return 0
    # ç­‰ç´š 1
    if action_group == "æ‰¾æœå‹™ï¼ˆå°‹æ±‚æœå‹™èˆ‡å®¢æœï¼‰":
        return 1
    # ç­‰ç´š 2
    if "å•†å“è³‡è¨Šé " in action_group or action_group == "å¥½åº·å„ªæƒ ":
        return 2
    # ç­‰ç´š 3
    combo3 = {
        "è‡ªç”±é…ï¼æŠ•è³‡è¦åŠƒ", "è‡ªç”±é…", "è¨‚è£½ä¿éšªçµ„åˆ", "è‡ªç”±é…ï¼å¥—é¤"
    }
    if action_group in combo3 or action_group.startswith("è©¦ç®—"):
        return 3
    # ç­‰ç´š 4
    result4 = {
        "è‡ªç”±é…ï¼ä¿éšœè¦åŠƒè©¦ç®—çµæœ",
        "è¨‚è£½ä¿éšªçµ„åˆï¼äººèº«è¦åŠƒè©¦ç®—çµæœ",
        "æˆ‘çš„ä¿éšªè©¦ç®—çµæœ",
        "è¨‚è£½ä¿éšªçµ„åˆï¼æŠ•è³‡è¦åŠƒè©¦ç®—çµæœ",
        "è‡ªç”±é…ï¼é…ç½®æˆ‘çš„åŸºé‡‘è©¦ç®—çµæœ"
    }
    if action_group in result4:
        return 4
    # ç­‰ç´š 5
    if action_group in {
        "ä¿å­˜èˆ‡åˆ†äº«è©¦ç®—çµæœ",
        "ä¿å­˜èˆ‡åˆ†äº«è‡ªç”±é…ã€è¨‚è£½çµ„åˆçµæœ",
        "Lineåˆ†äº«è½‰å‚³"
    }:
        return 5
    # ç­‰ç´š 6
    if action_group in {
        "æŒ‘é¸é ç´„é¡§å•",
        "é ç´„é¡§å•èˆ‡å•†å“è«®è©¢",
        "ç«‹å³æŠ•ä¿"
    }:
        return 6
    # ç­‰ç´š 7.x
    if action_group in {"æ–¹æ¡ˆç¢ºèª", "å¡«å¯«é ç´„è³‡æ–™"}:
        return 7.1
    if action_group in {"è³‡æ–™å¡«å¯«èˆ‡ç¢ºèª", "æŠ•ä¿è³‡æ ¼ç¢ºèª", "æ‰‹æ©Ÿé©—è­‰ç¢¼"}:
        return 7.2
    if action_group == "ç·šä¸Šç¹³è²»":
        return 7.3
    # ç­‰ç´š 8
    if action_group in {"å®Œæˆç¶²è·¯æŠ•ä¿", "å®ŒæˆO2O"}:
        return 8
    # æœªçŸ¥
    return -1

# 2. æ¨è–¦ç­–ç•¥å‡½å¼ï¼ˆä½¿ç”¨ next_action_group åƒæ•¸ï¼‰
def recommend_strategy(
        next_action_group: str,
        history: list[str],
        last_event_time: datetime
    ) -> str | None:
    # æª¢æŸ¥ next_action_group æ˜¯å¦ç‚º None æˆ–ç©ºå€¼
    if next_action_group is None or pd.isna(next_action_group) or next_action_group == '':
        return None
        
    # è™•ç†æ™‚å€å•é¡Œï¼šçµ±ä¸€è½‰æ›ç‚º naive datetime
    now = datetime.now()
    if hasattr(last_event_time, 'tz_localize'):
        # å¦‚æœæ˜¯ pandas Timestamp ä¸”æœ‰æ™‚å€ä¿¡æ¯ï¼Œè½‰æ›ç‚º naive
        if last_event_time.tz is not None:
            last_event_time = last_event_time.tz_localize(None)
    elif hasattr(last_event_time, 'tzinfo'):
        # å¦‚æœæ˜¯ datetime ä¸”æœ‰æ™‚å€ä¿¡æ¯ï¼Œè½‰æ›ç‚º naive
        if last_event_time.tzinfo is not None:
            last_event_time = last_event_time.replace(tzinfo=None)

    # ç­‰ç´š 0
    if next_action_group in {"å…¶ä»–", "ä¿éšªè¦–åœ–ã€ä¿å–®æ˜ç´°ã€è³‡ç”¢ç¸½è¦½ã€ä¿éšªæ˜ç´°"}:
        return None

    # ç­‰ç´š 1ï¼šæ‰¾æœå‹™
    if next_action_group == "æ‰¾æœå‹™ï¼ˆå°‹æ±‚æœå‹™èˆ‡å®¢æœï¼‰":
        s = "ç®­é ­æé†’ã€Œæ‰¾æœå‹™ã€ä»‹é¢åœ¨ç•«é¢ä¸Šæ–¹é¸å–®"
        if history[-10:].count("æ‰¾æœå‹™ï¼ˆå°‹æ±‚æœå‹™èˆ‡å®¢æœï¼‰") > 3:
            s += "ï¼›è®“å³ä¸‹è§’çš„é˜¿ç™¼çš„ã€ŒHi éœ€è¦å¹«å¿™å—ã€ç§€å‡ºä¾†"
        return s

    # ç­‰ç´š 2ï¼šå•†å“è³‡è¨Šé 
    if "å•†å“è³‡è¨Šé " in next_action_group:
        cnt = sum(1 for ag in history[-10:] if ag and "å•†å“è³‡è¨Šé " in ag)
        if cnt >= 3:
            return "é¡¯ç¤ºå…¥å£æ¨å‹•è©¦ç®—"
        return None

    # ç­‰ç´š 2ï¼šå¥½åº·å„ªæƒ 
    if next_action_group == "å¥½åº·å„ªæƒ ":
        if now - last_event_time > timedelta(minutes=3):
            return "å½ˆçª—é¡¯ç¤ºå¥½åº·å„ªæƒ ç›¸é—œè³‡è¨Š"
        return None

    # ç­‰ç´š 3ï¼šæŒ‘é¸çµ„åˆ
    combo3 = {"è‡ªç”±é…ï¼æŠ•è³‡è¦åŠƒ", "è‡ªç”±é…", "è¨‚è£½ä¿éšªçµ„åˆ", "è‡ªç”±é…ï¼å¥—é¤"}
    if next_action_group in combo3:
        prompt = (
            "ä¸çŸ¥é“å¦‚ä½•é–‹å§‹å—ï¼Ÿç°¡å–®ä¸‰å€‹å•é¡Œï¼Œè®“ç³»çµ±æä¾›æœ€é©åˆä½ çš„å•†å“ï¼"
            if next_action_group == "è¨‚è£½ä¿éšªçµ„åˆ"
            else "ä¸€éµæ­é…å€‹äººåŒ–çš„å•†å“çµ„åˆï¼Œåªè¦ 2 åˆ†é˜ï¼"
        )
        return f"å½ˆçª—ï¼šã€Œ{prompt}ã€"

    # ç­‰ç´š 3ï¼šè©¦ç®—
    if next_action_group.startswith("è©¦ç®—"):
        return "å½ˆçª—ï¼šã€Œä¸€éµå¸¶ä½ å®Œæˆè©¦ç®—ï¼Œåªè¦ 2 æ­¥é©Ÿï¼Œå–å¾—å•†å“è²»ç”¨ä¼°ç®—ã€"

    # ç­‰ç´š 4ï¼šè©¦ç®—çµæœ
    result4 = {
        "è‡ªç”±é…ï¼ä¿éšœè¦åŠƒè©¦ç®—çµæœ",
        "è¨‚è£½ä¿éšªçµ„åˆï¼äººèº«è¦åŠƒè©¦ç®—çµæœ",
        "æˆ‘çš„ä¿éšªè©¦ç®—çµæœ",
        "è¨‚è£½ä¿éšªçµ„åˆï¼æŠ•è³‡è¦åŠƒè©¦ç®—çµæœ",
        "è‡ªç”±é…ï¼é…ç½®æˆ‘çš„åŸºé‡‘è©¦ç®—çµæœ"
    }
    if next_action_group in result4:
        return "æä¾›é€²åº¦æé†’ï¼šå°±å¿«å®Œæˆäº†ï¼è©¦ç®—çµæœå³å°‡ç”¢ç”Ÿ"

    # ç­‰ç´š 5ï¼šä¿å­˜ï¼åˆ†äº«çµæœ
    if next_action_group in {
        "ä¿å­˜èˆ‡åˆ†äº«è©¦ç®—çµæœ",
        "ä¿å­˜èˆ‡åˆ†äº«è‡ªç”±é…ã€è¨‚è£½çµ„åˆçµæœ"
    }:
        return "è«®è©¢æŒ‰éˆ•æç¤ºï¼šã€Œå°è©¦ç®—çµæœæœ‰ç–‘å•å—ï¼Ÿé ç´„å…è²»å°ˆäººè§£è®€ï¼ã€"
    if next_action_group == "Lineåˆ†äº«è½‰å‚³":
        return "å½ˆçª—é¼“å‹µæ¨è–¦ã€æ¨æ’­æ¨è–¦çå‹µæ©Ÿåˆ¶æˆ–åˆ†äº«å›é¥‹æ´»å‹•"

    # ç­‰ç´š 6ï¼šæŒ‘é¡§å• / è«®è©¢éœ€æ±‚
    if next_action_group in {"æŒ‘é¸é ç´„é¡§å•", "é ç´„é¡§å•èˆ‡å•†å“è«®è©¢"}:
        return "å½ˆçª—æ¨è–¦é¡§å•ï¼šã€Œé€™ä½å°ˆå®¶æœ€æ“…é•·XXéšªï¼Œ3 åˆ†é˜å…§ç¢ºèªé ç´„ã€"
    if next_action_group == "ç«‹å³æŠ•ä¿":
        return "ç«‹å³æŠ•ä¿æŒ‰éˆ•æˆ–æ©«å¹…CTAï¼šã€Œç«‹å³æŠ•ä¿äº«å„ªæƒ ï¼ã€"

    # ç­‰ç´š 7.1ï¼šæ–¹æ¡ˆç¢ºèª / å¡«å¯«é ç´„è³‡æ–™
    if next_action_group in {"æ–¹æ¡ˆç¢ºèª", "å¡«å¯«é ç´„è³‡æ–™"}:
        if now - last_event_time <= timedelta(minutes=5):
            return (
                "é€²åº¦æé†’ï¼šã€Œå†ä¸‰æ­¥å³å¯å®ŒæˆæŠ•ä¿ã€"
                if next_action_group == "æ–¹æ¡ˆç¢ºèª"
                else "é€²åº¦æé†’ï¼šã€Œå†å…©æ­¥å³å¯å®Œæˆé ç´„ã€"
            )

    # ç­‰ç´š 7.2ï¼šè³‡æ–™å¡«å¯«èˆ‡ç¢ºèª / æŠ•ä¿è³‡æ ¼ç¢ºèª / æ‰‹æ©Ÿé©—è­‰ç¢¼
    if next_action_group in {"è³‡æ–™å¡«å¯«èˆ‡ç¢ºèª", "æŠ•ä¿è³‡æ ¼ç¢ºèª", "æ‰‹æ©Ÿé©—è­‰ç¢¼"}:
        if now - last_event_time > timedelta(minutes=30):
            return "æ©Ÿå™¨äººå¼•å°ï¼šã€Œä¸Šæ¬¡é‚„æ²’å¡«å®Œï¼Ÿé»æ­¤ä¸€éµå›åˆ°æµç¨‹ã€"
        if now - last_event_time <= timedelta(minutes=5):
            return (
                "é€²åº¦æé†’ï¼šã€Œé‚„å·®æœ€å¾Œä¸€æ­¥å°±OKï¼Œå³å°‡å®Œæˆé ç´„ã€"
                if next_action_group == "æ‰‹æ©Ÿé©—è­‰ç¢¼"
                else "é€²åº¦æé†’ï¼šã€Œé‚„å·®æœ€å¾Œå…©æ­¥å°±OKï¼Œå³å°‡å®ŒæˆæŠ•ä¿ã€"
            )

    # ç­‰ç´š 7.3ï¼šç·šä¸Šç¹³è²»
    if next_action_group == "ç·šä¸Šç¹³è²»":
        if now - last_event_time > timedelta(minutes=30):
            return "æ©Ÿå™¨äººå¼•å°ï¼šã€Œä¸Šæ¬¡é‚„æ²’å¡«å®Œï¼Ÿé»æ­¤ä¸€éµå›åˆ°æµç¨‹ã€"
        if now - last_event_time <= timedelta(minutes=5):
            return "é€²åº¦æé†’ï¼šã€Œé‚„å·®æœ€å¾Œä¸€æ­¥å°±OKï¼Œå³å°‡å®ŒæˆæŠ•ä¿ã€"

    # ç­‰ç´š 8ï¼šå®Œæˆç¶²æŠ• / å®ŒæˆO2O
    if next_action_group == "å®Œæˆç¶²è·¯æŠ•ä¿":
        if now - last_event_time > timedelta(minutes=30):
            return (
                "å¯„ç™¼EDMæé†’å³å°‡å®ŒæˆæŠ•ä¿ï¼Œä¸¦é™„ä¸Šé€£çµã€Œé‚„å·®æœ€å¾Œä¸€æ­¥å°±OKï¼Œ"
                "é»æˆ‘å›åˆ°æµç¨‹ã€"
            )
        return "æ„Ÿè¬é æ¨è–¦ï¼šã€Œæ„Ÿè¬æ‚¨ï¼é‚€è«‹å¥½å‹å®Œæˆç¶²æŠ•é€é›™æ–¹ NT$300å…ƒã€"
    if next_action_group == "å®ŒæˆO2O":
        if now - last_event_time > timedelta(minutes=30):
            return (
                "å¯„ç™¼EDMæé†’å³å°‡å®Œæˆé ç´„ï¼Œä¸¦é™„ä¸Šé€£çµã€Œé‚„å·®æœ€å¾Œä¸€æ­¥å°±OKï¼Œ"
                "é»æˆ‘å›åˆ°æµç¨‹ã€"
            )

    return None

# 3. æ ¹æ“š Top1~Top5 é¸å‡º next_action_group
def pick_next_action_group(row) -> str:
    candidates = []
    for i in range(1, 6):
        ag = row[f'Top{i}_next_action_group']
        conf = row[f'Top{i}_confidence']
        
        # æª¢æŸ¥ action_group æ˜¯å¦ç‚ºæœ‰æ•ˆå€¼
        if ag is None or pd.isna(ag) or ag == '':
            lvl = -1  # çµ¦ç„¡æ•ˆå€¼æœ€ä½ç­‰ç´š
        else:
            lvl = get_level(ag)
            
        candidates.append((ag, conf, lvl))
    
    # å…ˆæ¯”ç­‰ç´šï¼Œå†æ¯”æ©Ÿç‡ï¼Œéæ¿¾æ‰ç„¡æ•ˆå€¼
    valid_candidates = [(ag, conf, lvl) for ag, conf, lvl in candidates if lvl >= 0]
    
    if valid_candidates:
        best = max(valid_candidates, key=lambda x: (x[2], x[1]))
        return best[0]
    else:
        # å¦‚æœæ²’æœ‰æœ‰æ•ˆå€™é¸ï¼Œè¿”å›ç¬¬ä¸€å€‹éç©ºå€¼æˆ–é»˜èªå€¼
        for ag, conf, lvl in candidates:
            if ag is not None and not pd.isna(ag) and ag != '':
                return ag
        return "å…¶ä»–"  # é»˜èªå€¼

def safe_label_transform(encoder, value, default=0):
    """
    å®‰å…¨åœ°é€²è¡Œ label encodingï¼Œç¢ºä¿ä¸æœƒç”¢ç”Ÿè² å€¼
    """
    try:
        # è™•ç† NaN, None, ç©ºå­—ç¬¦ä¸²
        if pd.isna(value) or value is None or value == '' or value == 'nan':
            return default
        
        # è½‰æ›ç‚ºå­—ç¬¦ä¸²ï¼ˆä¿æŒèˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼‰
        str_value = str(value).strip()
        
        # å¦‚æœæ˜¯ç©ºå­—ç¬¦ä¸²ï¼Œè¿”å›é»˜èªå€¼
        if str_value == '' or str_value == 'nan':
            return default
        
        # æª¢æŸ¥æ˜¯å¦åœ¨å·²çŸ¥é¡åˆ¥ä¸­
        if str_value in encoder.classes_:
            result = encoder.transform([str_value])[0]
            # ç¢ºä¿çµæœä¸æ˜¯è² æ•¸
            if result < 0:
                print(f"è­¦å‘Šï¼šç·¨ç¢¼çµæœç‚ºè² æ•¸ {result}ï¼Œä½¿ç”¨é»˜èªå€¼ {default}")
                return default
            return result
        else:
            # æœªçŸ¥é¡åˆ¥ï¼Œè¿”å›é»˜èªå€¼
            print(f"è­¦å‘Šï¼šæœªçŸ¥é¡åˆ¥ '{str_value}' (åŸå€¼: {value})ï¼Œä½¿ç”¨é»˜èªå€¼ {default}")
            return default
            
    except Exception as e:
        print(f"Label encoding éŒ¯èª¤ (å€¼: {value}): {e}ï¼Œä½¿ç”¨é»˜èªå€¼ {default}")
        return default

def safe_scale(scaler, value, default=0.0):
    """
    å®‰å…¨åœ°é€²è¡Œæ•¸å€¼æ¨™æº–åŒ–
    """
    try:
        # è™•ç† NaN, None
        if pd.isna(value) or value is None:
            return default
        
        # è½‰æ›ç‚ºæµ®é»æ•¸
        float_value = float(value)
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰é™æ•¸
        if not np.isfinite(float_value):
            return default
        
        # é€²è¡Œæ¨™æº–åŒ–
        scaled_value = scaler.transform([[float_value]])[0][0]
        
        # æª¢æŸ¥çµæœæ˜¯å¦æœ‰æ•ˆ
        if not np.isfinite(scaled_value):
            return default
        
        return scaled_value
        
    except Exception as e:
        print(f"Scaling éŒ¯èª¤ (å€¼: {value}): {e}ï¼Œä½¿ç”¨é»˜èªå€¼ {default}")
        return default

def validate_and_fix_sequence(seq, seq_name):
    """
    é©—è­‰ä¸¦ä¿®å¾©åºåˆ—ä¸­çš„ç„¡æ•ˆå€¼
    """
    seq = np.array(seq)
    
    # æª¢æŸ¥ä¸¦ä¿®å¾©è² å€¼
    negative_mask = seq < 0
    if np.any(negative_mask):
        print(f"è­¦å‘Šï¼š{seq_name} ä¸­ç™¼ç¾ {np.sum(negative_mask)} å€‹è² å€¼ï¼Œå·²ä¿®å¾©ç‚º 0")
        seq[negative_mask] = 0
    
    # æª¢æŸ¥ä¸¦ä¿®å¾© NaN å€¼
    nan_mask = ~np.isfinite(seq)
    if np.any(nan_mask):
        print(f"è­¦å‘Šï¼š{seq_name} ä¸­ç™¼ç¾ {np.sum(nan_mask)} å€‹ç„¡æ•ˆå€¼ï¼Œå·²ä¿®å¾©ç‚º 0")
        seq[nan_mask] = 0
    
    return seq

def predict_from_uploaded_csv(df):
    if model is None or encoders is None or scalers is None:
        st.error("æ¨¡å‹æœªæ­£ç¢ºè¼‰å…¥ï¼Œç„¡æ³•é€²è¡Œé æ¸¬")
        return pd.DataFrame()
    
    # æª¢æŸ¥å¿…è¦æ¬„ä½
    required_columns = ['user_pseudo_id', 'event_time', 'platform', 'action', 'action_group'] \
                       + cat_features + num_features
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        st.error(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
        return pd.DataFrame()
    
    # è³‡æ–™é è™•ç†
    df = df.copy()
    df['event_time'] = pd.to_datetime(df['event_time'])
    
    # çµ±ä¸€è™•ç†æ™‚å€ï¼šå¦‚æœæœ‰æ™‚å€ä¿¡æ¯ï¼Œè½‰æ›ç‚º naive datetime
    if df['event_time'].dt.tz is not None:
        df['event_time'] = df['event_time'].dt.tz_localize(None)
    
    df = df.sort_values(by=["user_pseudo_id", "event_time"])
    
    results = []
    
    for user_id, group in df.groupby("user_pseudo_id"):
        try:
            # å–æœ€å¾Œ SEQ_LEN æ­¥
            last_steps = group.tail(SEQ_LEN)
            pad_len = SEQ_LEN - len(last_steps)
            
            # --- æ“·å– raw features ---
            last = last_steps.iloc[-1]
            raw_last_event_time   = last['event_time']
            raw_last_platform     = last['platform']
            raw_last_action       = last['action']
            raw_last_action_group = last['action_group']
            
            # å–å‰ 2~9 æ­¥çš„ raw action & action_group
            prev_records = {}
            for i in range(2, 10):
                if len(last_steps) >= i:
                    rec = last_steps.iloc[-i]
                    prev_records[f"-{i}_action"]        = rec['action']
                    prev_records[f"-{i}_action_group"]  = rec['action_group']
                else:
                    prev_records[f"-{i}_action"]        = None
                    prev_records[f"-{i}_action_group"]  = None
            
            # --- é¡åˆ¥ç‰¹å¾µç·¨ç¢¼ & å¡«å…… ---
            cat_inputs = []
            for col in cat_features:
                raw_vals = last_steps[col].tolist()
                encoded = [ safe_label_transform(encoders[col], v, default=0)
                            for v in raw_vals ]
                padded = [0]*pad_len + encoded
                seq = validate_and_fix_sequence(padded, f"{col}_sequence")
                cat_inputs.append(np.array(seq, dtype=np.int32).reshape(1, SEQ_LEN))
            
            # --- æ•¸å€¼ç‰¹å¾µæ¨™æº–åŒ– & å¡«å…… ---
            num_inputs = []
            for col in num_features:
                raw_vals = last_steps[col].tolist()
                scaled = [ safe_scale(scalers[col], v, default=0.0)
                           for v in raw_vals ]
                padded = [0.0]*pad_len + scaled
                seq = validate_and_fix_sequence(padded, f"{col}_sequence")
                num_inputs.append(np.array(seq, dtype=np.float32).reshape(1, SEQ_LEN))
            
            all_inputs = cat_inputs + num_inputs

            # é€²è¡Œé æ¸¬ä¸¦å– Top5
            y_pred_action_group, y_pred_online, y_pred_o2o = model.predict(all_inputs, verbose=0)
            top5_idx   = y_pred_action_group[0].argsort()[-5:][::-1]
            top5_confs = y_pred_action_group[0][top5_idx]
            inv_ag     = {i: v for i, v in enumerate(encoders['action_group'].classes_)}
            top5_actions = [inv_ag.get(idx, "æœªçŸ¥") for idx in top5_idx]
            
            # æ­£ç¢ºåœ° build ä¸­ä»‹ dictï¼Œæ³¨æ„ for è¿´åœˆå¾Œè¦æœ‰å†’è™Ÿ
            temp = {}
            for i in range(5):
                temp[f"Top{i+1}_next_action_group"] = top5_actions[i]
                temp[f"Top{i+1}_confidence"]       = round(float(top5_confs[i]), 4)
                
            # å†æŠŠé€™å€‹ dict å‚³çµ¦ pick_next_action_group
            next_ag = pick_next_action_group(temp)
            
            # è¨ˆç®—è¡ŒéŠ·ç­–ç•¥
            strategy = recommend_strategy(
                next_ag,
                [ prev_records.get(f"-{i}_action_group") for i in range(1, 10) ],
                raw_last_event_time
            )
            
            # --- çµ„è£çµæœ ---
            result = {
                # ğŸ”‘ åŸºæœ¬èº«ä»½ä¿¡æ¯
                "user_pseudo_id": user_id,
                
                # ğŸ¯ å®Œæ•´Top5é æ¸¬ (æœ€é‡è¦ï¼Œæ”¾åœ¨å‰é¢)
                "Top1_next_action_group": top5_actions[0],
                "Top1_confidence": round(float(top5_confs[0]), 4),
                "Top2_next_action_group": top5_actions[1],
                "Top2_confidence": round(float(top5_confs[1]), 4),
                "Top3_next_action_group": top5_actions[2],
                "Top3_confidence": round(float(top5_confs[2]), 4),
                "Top4_next_action_group": top5_actions[3],
                "Top4_confidence": round(float(top5_confs[3]), 4),
                "Top5_next_action_group": top5_actions[4],
                "Top5_confidence": round(float(top5_confs[4]), 4),
                
                # ğŸ“Š è½‰æ›æ©Ÿç‡å’Œç­–ç•¥
                "Online_conversion_prob": round(float(y_pred_online[0][0]), 4),
                "O2O_reservation_prob": round(float(y_pred_o2o[0][0]), 4),
                "Marketing_Strategy": strategy,
                
                # ğŸ“± ç•¶å‰è¡Œç‚ºä¿¡æ¯
                "last_event_time": raw_last_event_time,
                "last_platform": raw_last_platform,
                "last_action": raw_last_action,
                "last_action_group": raw_last_action_group,
                
                # ğŸ“š æ­·å²è¡Œç‚ºè¨˜éŒ„
                **prev_records
            }
            
            results.append(result)
        
        except Exception as e:
            st.error(f"è™•ç†ç”¨æˆ¶ {user_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
    
    return pd.DataFrame(results)

# UI ä»‹é¢
st.title("ğŸ“Š åœ‹æ³°äººå£½ â€“ å¤šå…ƒè¨ªå®¢è¡Œç‚ºé æ¸¬å·¥å…·")

# è¼‰å…¥æ¨¡å‹ä¸¦é¡¯ç¤ºç‹€æ…‹
with st.spinner("æ­£åœ¨è¼‰å…¥æ¨¡å‹..."):
    if TF_AVAILABLE:
        model, encoders, scalers = load_model_and_encoders()
        if model is not None:
            st.success("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        else:
            st.error("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—")
            st.stop()
    else:
        st.error("âš ï¸ TensorFlow ä¸å¯ç”¨")
        st.stop()

# ==== æ­¥é©Ÿ 1: æ–‡ä»¶ä¸Šå‚³ ====
st.markdown("### ğŸ“¥ æ­¥é©Ÿ 1: ä¸Šå‚³æ•¸æ“šæ–‡ä»¶")
uploaded_file = st.file_uploader(
    "ä¸Šå‚³åŒ…å«ç”¨æˆ¶è¡Œç‚ºæ­·ç¨‹è³‡æ–™çš„ CSV æ–‡ä»¶",
    type=["csv"],
    help="éœ€åŒ…å«æ¬„ä½ï¼šuser_pseudo_id, event_time, action_group, source, medium, platform, staytime, has_shared, revisit_count"
)

# é¡¯ç¤ºä¸Šå‚³ç‹€æ…‹
if uploaded_file is not None:
    try:
        user_df = pd.read_csv(uploaded_file)
        st.session_state.raw_uploaded_data = user_df
        st.success(f"âœ… æ–‡ä»¶ä¸Šå‚³æˆåŠŸï¼å…± {len(user_df)} ç­†è³‡æ–™")
        
        # é è¦½æ•¸æ“š
        with st.expander("ğŸ“Š æ•¸æ“šé è¦½", expanded=False):
            st.dataframe(user_df.head(10), use_container_width=True)
            
    except Exception as e:
        st.error(f"âŒ æ–‡ä»¶è®€å–éŒ¯èª¤: {e}")
        st.session_state.raw_uploaded_data = None

# ==== æ­¥é©Ÿ 2: æ—¥æœŸç¯„åœé¸æ“‡ ====
st.markdown("### ğŸ“… æ­¥é©Ÿ 2: é¸æ“‡åˆ†ææœŸé–“")

if st.session_state.raw_uploaded_data is not None:
    user_df = st.session_state.raw_uploaded_data
    user_df['event_time'] = pd.to_datetime(user_df['event_time'])
    
    # ç²å–è³‡æ–™çš„æ™‚é–“ç¯„åœ
    min_date = user_df['event_time'].min().date()
    max_date = user_df['event_time'].max().date()
    
    col1, col2 = st.columns(2)
    with col1:
        start_date = st.date_input(
            "èµ·å§‹æ—¥æœŸ", 
            value=min_date,
            min_value=min_date,
            max_value=max_date,
            key="start_date"
        )
    
    with col2:
        end_date = st.date_input(
            "æˆªæ­¢æ—¥æœŸ", 
            value=max_date,
            min_value=min_date,
            max_value=max_date,
            key="end_date"
        )
    
    # é©—è­‰æ—¥æœŸç¯„åœ
    if start_date <= end_date:
        filtered_df = user_df[
            (user_df['event_time'].dt.date >= start_date) & 
            (user_df['event_time'].dt.date <= end_date)
        ]
        st.info(f"ğŸ“Š é¸å®šæœŸé–“å…§è³‡æ–™: {len(filtered_df)} ç­† ({start_date} ~ {end_date})")
        
        # ä¿å­˜ç¯©é¸å¾Œçš„è³‡æ–™
        st.session_state.filtered_input_data = filtered_df
    else:
        st.error("âŒ èµ·å§‹æ—¥æœŸä¸èƒ½å¤§æ–¼æˆªæ­¢æ—¥æœŸ")
        st.session_state.filtered_input_data = None
else:
    st.info("ğŸ“¤ è«‹å…ˆä¸Šå‚³æ•¸æ“šæ–‡ä»¶")
    col1, col2 = st.columns(2)
    with col1:
        st.date_input("èµ·å§‹æ—¥æœŸ", disabled=True)
    with col2:
        st.date_input("æˆªæ­¢æ—¥æœŸ", disabled=True)

# ==== æ­¥é©Ÿ 3: é–‹å§‹é æ¸¬ ====
st.markdown("### ğŸš€ æ­¥é©Ÿ 3: é–‹å§‹é æ¸¬")

# é æ¸¬æŒ‰éˆ•
prediction_ready = (st.session_state.raw_uploaded_data is not None and 
                   'filtered_input_data' in st.session_state and 
                   st.session_state.filtered_input_data is not None)

if prediction_ready:
    if st.button("ğŸš€ é–‹å§‹é æ¸¬", type="primary", use_container_width=True):
        with st.spinner("ğŸ”„ æ­£åœ¨é€²è¡Œæ¨¡å‹é æ¸¬ï¼Œè«‹ç¨å€™..."):
            df_result = predict_from_uploaded_csv(st.session_state.filtered_input_data)
            
        if not df_result.empty:
            st.session_state.prediction_data = df_result
            st.session_state.all_columns = df_result.columns.tolist()
            st.success("ğŸ‰ é æ¸¬å®Œæˆï¼")
            # ç§»é™¤æ°£çƒæ•ˆæœ
        else:
            st.error("âŒ é æ¸¬å¤±æ•—ï¼Œè«‹æª¢æŸ¥è³‡æ–™æ ¼å¼")
else:
    st.button("ğŸš€ é–‹å§‹é æ¸¬", disabled=True, help="è«‹å…ˆå®Œæˆå‰é¢æ­¥é©Ÿ")
    if not prediction_ready:
        st.info("ğŸ“‹ å®Œæˆæ–‡ä»¶ä¸Šå‚³å’Œæ—¥æœŸé¸æ“‡å¾Œå³å¯é–‹å§‹é æ¸¬")

# ==== æ­¥é©Ÿ 4: é æ¸¬çµæœç¸½è¦½ ====
st.markdown("### ğŸ“‹ æ­¥é©Ÿ 4: é æ¸¬çµæœç¸½è¦½")

if st.session_state.prediction_data is not None:
    df = st.session_state.prediction_data
    
    # é¡¯ç¤ºç¸½è¦½çµ±è¨ˆ
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ğŸ“Š ç¸½ç”¨æˆ¶æ•¸", len(df))
    with col2:
        avg_confidence = df['Top1_confidence'].mean()
        st.metric("ğŸ¯ å¹³å‡Top1ä¿¡å¿ƒ", f"{avg_confidence:.3f}")
    with col3:
        # é«˜è½‰æ›æ©Ÿç‡ç”¨æˆ¶ - ç¶²æŠ•
        high_online_users = len(df[df['Online_conversion_prob'] >= 0.3])
        online_rate = high_online_users / len(df) * 100
        st.metric("ğŸ’» è½‰æ›æ©Ÿç‡ â‰¥0.3 ç”¨æˆ¶ - ç¶²æŠ•", f"{high_online_users} ({online_rate:.1f}%)")
    with col4:
        # é«˜è½‰æ›æ©Ÿç‡ç”¨æˆ¶ - é ç´„O2O
        high_o2o_users = len(df[df['O2O_reservation_prob'] >= 0.3])
        o2o_rate = high_o2o_users / len(df) * 100
        st.metric("ğŸ¤ è½‰æ›æ©Ÿç‡ â‰¥0.3 ç”¨æˆ¶ - é ç´„O2O", f"{high_o2o_users} ({o2o_rate:.1f}%)")
    
    # é æ¸¬çµæœé è¦½
    with st.expander("ğŸ“Š æŸ¥çœ‹å®Œæ•´é æ¸¬çµæœ", expanded=False):
        st.dataframe(df, use_container_width=True)
        
else:
    st.info("ğŸ“¤ å®Œæˆé æ¸¬å¾Œå°‡åœ¨æ­¤é¡¯ç¤ºçµæœç¸½è¦½")
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ğŸ“Š ç¸½ç”¨æˆ¶æ•¸", "---")
    with col2:
        st.metric("ğŸ¯ å¹³å‡Top1ä¿¡å¿ƒåˆ†æ•¸", "---")
    with col3:
        st.metric("ğŸ’» æ›æ©Ÿç‡ â‰¥0.3 ç”¨æˆ¶ - ç¶²æŠ•", "---")
    with col4:
        st.metric("ğŸ¤ è½‰æ›æ©Ÿç‡ â‰¥0.3 ç”¨æˆ¶ - é ç´„O2O", "---")

# ==== æ­¥é©Ÿ 5: ç¯©é¸ä¸‹è¼‰æ¢ä»¶ ====
st.markdown("### ğŸ¯ æ­¥é©Ÿ 5: è¨­å®šç¯©é¸æ¢ä»¶")

# ç¯©é¸å™¨å€åŸŸ
if st.session_state.prediction_data is not None:
    df = st.session_state.prediction_data
    
    with st.expander("ğŸ” è¨­å®šç¯©é¸æ¢ä»¶", expanded=True):
        col1, col2 = st.columns(2)
        
        with col1:
            # 1ï¸âƒ£ æ­·å²è¡Œç‚ºç¯©é¸
            st.markdown("**1ï¸âƒ£ æ­·å²è¡Œç‚ºç¯©é¸**")
            history_steps = st.selectbox("æœ€è¿‘ï¼®æ­¥å…§", options=list(range(2, 10)), index=6)
            
            # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„ action_group
            history_columns = [f"-{i}_action_group" for i in range(2, 10)]
            all_history_actions = set()
            for col in history_columns:
                if col in df.columns:
                    all_history_actions.update(df[col].dropna().unique())
            all_history_actions = sorted([x for x in all_history_actions if pd.notna(x)])
            
            selected_history_actions = st.multiselect(
                "æ›¾åŸ·è¡Œä»¥ä¸‹è¡Œç‚º",
                options=all_history_actions,
                help="é¸æ“‡ç”¨æˆ¶åœ¨æ­·å²ä¸­æ›¾ç¶“åŸ·è¡Œéçš„è¡Œç‚º"
            )
        
        with col2:
            # 2ï¸âƒ£ é æ¸¬è¡Œç‚ºç¯©é¸
            st.markdown("**2ï¸âƒ£ é æ¸¬è¡Œç‚ºç¯©é¸**")
            top_n = st.selectbox("é æ¸¬ä¸‹ä¸€æ­¥çš„Topï¼®ä¸­", options=[1, 2, 3, 4, 5], index=0)
            
            # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„é æ¸¬ action_group
            prediction_columns = [f"Top{i}_next_action_group" for i in range(1, 6)]
            all_prediction_actions = set()
            for col in prediction_columns:
                if col in df.columns:
                    all_prediction_actions.update(df[col].dropna().unique())
            all_prediction_actions = sorted([x for x in all_prediction_actions if pd.notna(x)])
            
            selected_prediction_actions = st.multiselect(
                "åŒ…å«ä»¥ä¸‹è¡Œç‚º",
                options=all_prediction_actions,
                help="é¸æ“‡é æ¸¬çš„ä¸‹ä¸€æ­¥è¡Œç‚º"
            )
        
        # 3ï¸âƒ£ Top1 æ©Ÿç‡é–€æª»
        st.markdown("**3ï¸âƒ£ é æ¸¬ä¿¡å¿ƒé–€æª»**")
        
        # æä¾›é è¨­å»ºè­°å€¼
        confidence_option = st.radio(
            "é¸æ“‡ä¿¡å¿ƒé–€æª»ç­–ç•¥",
            ["è‡ªå®šç¾©", "ä¿å®ˆç­–ç•¥(Top1â‰¥0.4)", "å¹³è¡¡ç­–ç•¥(Top1â‰¥0.3)", "ç©æ¥µç­–ç•¥(Top1â‰¥0.2)"],
            help="æ ¹æ“šæ¨¡å‹æº–ç¢ºåº¦ï¼šTop1â‰ˆ70%, Top3â‰ˆ85%, Top5â‰ˆ93%"
        )
        
        if confidence_option == "ä¿å®ˆç­–ç•¥(Top1â‰¥0.4)":
            min_confidence = 0.4
            st.info("ğŸ›¡ï¸ ä¿å®ˆç­–ç•¥ï¼šå„ªå…ˆé¸æ“‡é«˜ä¿¡å¿ƒé æ¸¬ï¼Œé™ä½èª¤åˆ¤é¢¨éšª")
        elif confidence_option == "å¹³è¡¡ç­–ç•¥(Top1â‰¥0.3)":
            min_confidence = 0.3
            st.info("âš–ï¸ å¹³è¡¡ç­–ç•¥ï¼šåœ¨æº–ç¢ºåº¦å’Œè¦†è“‹ç‡é–“å–å¾—å¹³è¡¡")
        elif confidence_option == "ç©æ¥µç­–ç•¥(Top1â‰¥0.2)":
            min_confidence = 0.2
            st.info("ğŸš€ ç©æ¥µç­–ç•¥ï¼šæœ€å¤§åŒ–è§¸åŠç”¨æˆ¶æ•¸ï¼Œé©åˆæ¢ç´¢æ€§ç‡ŸéŠ·")
        else:
            min_confidence = st.number_input(
                "Top1 æœ€ä½æ©Ÿç‡",
                min_value=0.0,
                max_value=1.0,
                value=0.3,
                step=0.01,
                help="å»ºè­°å€¼ï¼š0.2-0.4 ä¹‹é–“ï¼Œè€ƒæ…®æ¨¡å‹æº–ç¢ºåº¦å¹³è¡¡"
            )
        
        # 4ï¸âƒ£ æ¬„ä½é¸æ“‡
        st.markdown("**4ï¸âƒ£ é¸æ“‡è¼¸å‡ºæ¬„ä½**")
        
        # æä¾›å¿«é€Ÿé¸é …
        col1, col2, col3 = st.columns(3)
        with col1:
            if st.button("ğŸ”„ å…¨é¸", key="select_all"):
                st.session_state.selected_columns = st.session_state.all_columns
        with col2:
            if st.button("ğŸ“Š æ ¸å¿ƒæ¬„ä½", key="select_core"):
                # æŒ‰æ¥­å‹™é‡è¦æ€§æ’åºçš„æ ¸å¿ƒæ¬„ä½
                core_columns = [
                    'user_pseudo_id', 
                    'Top1_next_action_group', 'Top1_confidence',
                    'Top2_next_action_group', 'Top2_confidence',
                    'Top3_next_action_group', 'Top3_confidence',
                    'Online_conversion_prob', 'O2O_reservation_prob', 
                    'Marketing_Strategy', 'last_event_time'
                ]
                st.session_state.selected_columns = [col for col in core_columns if col in st.session_state.all_columns]
        with col3:
            if st.button("ğŸ¯ é æ¸¬æ¬„ä½", key="select_prediction"):
                prediction_cols = [
                    'user_pseudo_id',
                    'Top1_next_action_group', 'Top1_confidence',
                    'Top2_next_action_group', 'Top2_confidence', 
                    'Top3_next_action_group', 'Top3_confidence',
                    'Top4_next_action_group', 'Top4_confidence',
                    'Top5_next_action_group', 'Top5_confidence',
                    'Online_conversion_prob', 'O2O_reservation_prob'
                ]
                st.session_state.selected_columns = [col for col in prediction_cols if col in st.session_state.all_columns]

        # æ¬„ä½å¤šé¸å™¨
        if 'selected_columns' not in st.session_state:
            st.session_state.selected_columns = st.session_state.all_columns
        
        selected_columns = st.multiselect(
            "é¸æ“‡è¦è¼¸å‡ºçš„æ¬„ä½",
            options=st.session_state.all_columns,
            default=st.session_state.selected_columns,
            key="column_selector"
        )
        
        # è‡ªå‹•æ›´æ–°é¸ä¸­çš„æ¬„ä½
        if selected_columns != st.session_state.selected_columns:
            st.session_state.selected_columns = selected_columns
            st.rerun()

else:
    st.info("ğŸ“¤ å®Œæˆé æ¸¬å¾Œå³å¯ç¯©é¸çµæœ")
    with st.expander("ğŸ” ç¯©é¸æ¢ä»¶é è¦½", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**æ­·å²è¡Œç‚ºç¯©é¸**")
            st.selectbox("æœ€è¿‘å¹¾æ­¥å…§", options=list(range(1, 11)), disabled=True)
            st.multiselect("æ›¾åŸ·è¡Œ", options=[], disabled=True)
        with col2:
            st.markdown("**é æ¸¬è¡Œç‚ºç¯©é¸**")
            st.selectbox("é æ¸¬ä¸‹ä¸€æ­¥çš„çµæœä¸­ï¼ŒTopå¹¾åŒ…å«", options=[1, 2, 3, 4, 5], disabled=True)
            st.multiselect("åŒ…å«", options=[], disabled=True)
        
        st.markdown("**é æ¸¬ä¿¡å¿ƒé–€æª»**")
        st.radio("é¸æ“‡ä¿¡å¿ƒé–€æª»ç­–ç•¥", ["è‡ªå®šç¾©", "ä¿å®ˆç­–ç•¥", "å¹³è¡¡ç­–ç•¥", "ç©æ¥µç­–ç•¥"], disabled=True)
        
        st.markdown("**é¸æ“‡è¼¸å‡ºæ¬„ä½**")
        st.multiselect("é¸æ“‡è¦è¼¸å‡ºçš„æ¬„ä½", options=[], disabled=True)

# ==== æ­¥é©Ÿ 6: ç¯©é¸æ¢ä»¶ç¸½çµèˆ‡ä¸‹è¼‰ ====
st.markdown("### ğŸ“‹ æ­¥é©Ÿ 6: ç¢ºèªæ¢ä»¶ä¸¦ä¸‹è¼‰")

if st.session_state.prediction_data is not None:
    df = st.session_state.prediction_data
    
    # å¥—ç”¨ç¯©é¸é‚è¼¯
    filtered_df = df.copy()
    filter_conditions = []
    
    # 1ï¸âƒ£ æ­·å²è¡Œç‚ºç¯©é¸
    if 'selected_history_actions' in locals() and selected_history_actions:
        history_mask = pd.Series([False] * len(filtered_df), index=filtered_df.index)
        for idx, row in filtered_df.iterrows():
            # æª¢æŸ¥æœ€è¿‘ history_steps æ­¥å…§çš„è¡Œç‚º
            for step in range(1, min(history_steps + 1, max_history_steps + 1)):
                if step == 1:
                    # ç•¶å‰è¡Œç‚º
                    if row['last_action_group'] in selected_history_actions:
                        history_mask[idx] = True
                        break
                else:
                    # æ­·å²è¡Œç‚º
                    col_name = f"-{step}_action_group"
                    if col_name in row and pd.notna(row[col_name]) and row[col_name] in selected_history_actions:
                        history_mask[idx] = True
                        break
        filtered_df = filtered_df[history_mask]
        
        actions_text = "ã€".join(selected_history_actions)
        filter_conditions.append(f"æœ€è¿‘{history_steps}æ­¥å…§æ›¾åŸ·è¡Œï¼š{actions_text}")
    
    # 2ï¸âƒ£ é æ¸¬è¡Œç‚ºç¯©é¸  
    if 'selected_prediction_actions' in locals() and selected_prediction_actions:
        prediction_mask = pd.Series([False] * len(filtered_df), index=filtered_df.index)
        for idx, row in filtered_df.iterrows():
            for n in range(1, top_n + 1):
                col_name = f"Top{n}_next_action_group"
                if col_name in row and row[col_name] in selected_prediction_actions:
                    prediction_mask[idx] = True
                    break
        filtered_df = filtered_df[prediction_mask]
        
        actions_text = "ã€".join(selected_prediction_actions)
        filter_conditions.append(f"é æ¸¬ä¸‹ä¸€æ­¥çš„çµæœä¸­ï¼ŒTop{top_n}åŒ…å«ï¼š{actions_text}")
    
    # 3ï¸âƒ£ Top1 æ©Ÿç‡é–€æª»ç¯©é¸
    if 'min_confidence' in locals() and min_confidence > 0.0:
        filtered_df = filtered_df[filtered_df['Top1_confidence'] >= min_confidence]
        filter_conditions.append(f"Top1çš„æœ€ä½æ©Ÿç‡éœ€å¤§æ–¼ç­‰æ–¼ï¼š{min_confidence}")
    
    # é¡¯ç¤ºç¯©é¸æ¢ä»¶ç¸½çµ
    if filter_conditions:
        if 'start_date' in locals() and 'end_date' in locals():
            date_range = f"åœ¨ {start_date} ~ {end_date} ä¹‹é–“"
        else:
            date_range = "åœ¨æ‰€é¸æ—¥æœŸç¯„åœå…§"
        
        conditions_text = f"""
        **æ‚¨çš„ç¯©é¸æ¢ä»¶ç‚ºï¼š**
        
        {date_range}ï¼Œ
        """
        
        for i, condition in enumerate(filter_conditions):
            if i == 0:
                conditions_text += f"{condition}"
            else:
                conditions_text += f"\nä¸” {condition}"
        
        conditions_text += f"""
        
        **ç•¶å‰æ¢ä»¶ä¸‹ï¼Œå…¨æ•¸ç¬¦åˆçš„ç”¨æˆ¶æ•¸é‡å…±æœ‰ï¼š{len(filtered_df)} äºº**
        """
        
        st.markdown(conditions_text)
    else:
        st.info("ğŸ” ç›®å‰æœªè¨­å®šä»»ä½•ç¯©é¸æ¢ä»¶ï¼Œå°‡ä¸‹è¼‰æ‰€æœ‰é æ¸¬çµæœ")
        st.markdown(f"**ç”¨æˆ¶ç¸½æ•¸ï¼š{len(filtered_df)} äºº**")
    
    # ä¸‹è¼‰æŒ‰éˆ•
    if len(filtered_df) > 0 and 'selected_columns' in st.session_state and st.session_state.selected_columns:
        csv = filtered_df[st.session_state.selected_columns].to_csv(index=False).encode("utf-8-sig")
        
        # ç”Ÿæˆæª”å
        if filter_conditions:
            filename = f"prediction_result_filtered_{len(filtered_df)}users.csv"
        else:
            filename = f"prediction_result_all_{len(filtered_df)}users.csv"
        
        st.download_button(
            "ğŸ“¥ ç«‹å³ä¸‹è¼‰ CSV",
            data=csv,
            file_name=filename,
            mime="text/csv",
            type="primary",
            use_container_width=True,
            help=f"ä¸‹è¼‰ {len(filtered_df)} ä½ç”¨æˆ¶çš„ç¯©é¸çµæœï¼ŒåŒ…å« {len(st.session_state.selected_columns)} å€‹æ¬„ä½"
        )
        
        # é¡¯ç¤ºç¯©é¸å¾Œçš„è³‡æ–™é è¦½
        with st.expander("ğŸ“Š ä¸‹è¼‰å…§å®¹é è¦½", expanded=False):
            st.dataframe(filtered_df[st.session_state.selected_columns], use_container_width=True)
            
    elif len(filtered_df) == 0:
        st.error("âŒ æ²’æœ‰ç¬¦åˆç¯©é¸æ¢ä»¶çš„è³‡æ–™ï¼Œç„¡æ³•ä¸‹è¼‰")
    elif not st.session_state.selected_columns:
        st.warning("âš ï¸ è«‹å…ˆé¸æ“‡è¦è¼¸å‡ºçš„æ¬„ä½")
    else:
        st.button("ğŸ“¥ ç«‹å³ä¸‹è¼‰ CSV", disabled=True, help="è«‹å®Œæˆæ‰€æœ‰è¨­å®š")

else:
    st.info("ğŸ“¤ å®Œæˆé æ¸¬å’Œç¯©é¸è¨­å®šå¾Œå³å¯ä¸‹è¼‰çµæœ")
    st.button("ğŸ“¥ ç«‹å³ä¸‹è¼‰ CSV", disabled=True)

# ==== çµ±è¨ˆåœ–è¡¨å€åŸŸ ====
st.markdown("### ğŸ“Š æ•¸æ“šåˆ†æåœ–è¡¨")

if st.session_state.prediction_data is not None and 'filtered_df' in locals() and len(filtered_df) > 0:
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š è¡Œç‚ºåˆ†ä½ˆ", "ğŸ“ˆ ä¿¡å¿ƒåˆ†æ•¸", "ğŸ” è½‰æ›åˆ†æ", "ğŸ¯ ç­–ç•¥åˆ†ä½ˆ"])
    
    with tab1:
        chart_df = filtered_df["Top1_next_action_group"].value_counts().reset_index()
        chart_df.columns = ["action_group", "count"]
        fig1 = px.bar(chart_df, x="action_group", y="count", title="Top1 é æ¸¬è¡Œç‚ºåˆ†ä½ˆ")
        fig1.update_layout(xaxis_tickangle=-45)
        st.plotly_chart(fig1, use_container_width=True)
    
    with tab2:
        fig2 = px.histogram(filtered_df, x="Top1_confidence", nbins=20, title="Top1 é æ¸¬ä¿¡å¿ƒåˆ†æ•¸åˆ†ä½ˆ")
        st.plotly_chart(fig2, use_container_width=True)
    
    with tab3:
        fig3 = px.scatter(
            filtered_df, 
            x="Top1_confidence", 
            y="Online_conversion_prob", 
            color="O2O_reservation_prob",
            title="é æ¸¬ä¿¡å¿ƒ vs è½‰æ›æ©Ÿç‡åˆ†æ",
            hover_data=['user_pseudo_id']
        )
        st.plotly_chart(fig3, use_container_width=True)
    
    with tab4:
        strategy_df = filtered_df["Marketing_Strategy"].value_counts().reset_index()
        strategy_df.columns = ["strategy", "count"]
        strategy_df = strategy_df[strategy_df['strategy'].notna()]  # éæ¿¾æ‰ None å€¼
        if len(strategy_df) > 0:
            fig4 = px.pie(strategy_df, names="strategy", values="count", title="å»ºè­°è¡ŒéŠ·ç­–ç•¥åˆ†ä½ˆ")
            st.plotly_chart(fig4, use_container_width=True)
        else:
            st.info("æš«ç„¡è¡ŒéŠ·ç­–ç•¥å»ºè­°è³‡æ–™")

else:
    st.info("ğŸ“Š å®Œæˆé æ¸¬å¾Œï¼Œé€™è£¡å°‡é¡¯ç¤ºè©³ç´°çš„æ•¸æ“šåˆ†æåœ–è¡¨")
    
    # é è¦½åœ–è¡¨çµæ§‹
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š è¡Œç‚ºåˆ†ä½ˆ", "ğŸ“ˆ ä¿¡å¿ƒåˆ†æ•¸", "ğŸ” è½‰æ›åˆ†æ", "ğŸ¯ ç­–ç•¥åˆ†ä½ˆ"])
    
    with tab1:
        st.markdown("**Top1 é æ¸¬è¡Œç‚ºåˆ†ä½ˆåœ–**")
        st.info("å°‡é¡¯ç¤ºå„ç¨®é æ¸¬è¡Œç‚ºçš„ç”¨æˆ¶æ•¸é‡åˆ†ä½ˆ")
    
    with tab2:
        st.markdown("**Top1 é æ¸¬ä¿¡å¿ƒåˆ†æ•¸åˆ†ä½ˆ**")
        st.info("å°‡é¡¯ç¤ºæ¨¡å‹é æ¸¬ä¿¡å¿ƒåˆ†æ•¸çš„åˆ†ä½ˆæƒ…æ³")
    
    with tab3:
        st.markdown("**é æ¸¬ä¿¡å¿ƒ vs è½‰æ›æ©Ÿç‡åˆ†æ**")
        st.info("å°‡é¡¯ç¤ºé æ¸¬ä¿¡å¿ƒèˆ‡ç·šä¸Š/O2Oè½‰æ›æ©Ÿç‡çš„é—œä¿‚")
    
    with tab4:
        st.markdown("**å»ºè­°è¡ŒéŠ·ç­–ç•¥åˆ†ä½ˆ**")
        st.info("å°‡é¡¯ç¤ºç³»çµ±å»ºè­°çš„å„ç¨®è¡ŒéŠ·ç­–ç•¥æ¯”ä¾‹")
