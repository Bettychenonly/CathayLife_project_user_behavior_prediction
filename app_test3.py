import streamlit as st

# ğŸš¨ é‡è¦ï¼šst.set_page_config å¿…é ˆæ˜¯ç¬¬ä¸€å€‹ Streamlit å‘½ä»¤
st.set_page_config(page_title="åœ‹æ³°äººå£½ - ç”¨æˆ¶è¡Œç‚ºé æ¸¬å·¥å…·", layout="wide", initial_sidebar_state="collapsed")

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import plotly.express as px

# å˜—è©¦è¼‰å…¥ TensorFlow å’Œç›¸é—œå¥—ä»¶
try:
    from tensorflow.keras.models import load_model
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from joblib import load
    TF_AVAILABLE = True
except ImportError as e:
    st.error(f"è¼‰å…¥ä¾è³´å¥—ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    TF_AVAILABLE = False

# åˆå§‹åŒ– session state
if "prediction_data" not in st.session_state:
    st.session_state.prediction_data = None
if "all_columns" not in st.session_state:
    st.session_state.all_columns = []
if "uploaded_file_name" not in st.session_state:
    st.session_state.uploaded_file_name = None
if "processed_data" not in st.session_state:
    st.session_state.processed_data = None

# æ¨¡å‹èˆ‡ç·¨ç¢¼å™¨è¼‰å…¥
SEQ_LEN = 10
cat_features = ['action_group', 'source', 'medium', 'platform']
num_features = ['staytime', 'has_shared', 'revisit_count']

@st.cache_resource
def load_model_and_encoders():
   if not TF_AVAILABLE:
       st.error("TensorFlow æœªæ­£ç¢ºå®‰è£ï¼Œç„¡æ³•è¼‰å…¥æ¨¡å‹")
       return None, None, None
   
   # è¨ºæ–·æª”æ¡ˆç‹€æ…‹
   import os
   model_file = "lstm_multi_output_model_v2.h5"
   
   if os.path.exists(model_file):
       file_size = os.path.getsize(model_file)
       st.info(f"æ¨¡å‹æª”æ¡ˆå­˜åœ¨ï¼Œå¤§å°: {file_size} bytes ({file_size/1024/1024:.2f} MB)")
   
   try:
       # å…ˆè¼‰å…¥æ¨¡å‹
       st.info("æ­£åœ¨è¼‰å…¥ LSTM æ¨¡å‹...")
       model = load_model("lstm_multi_output_model_v2.h5")
       st.success("âœ… LSTM æ¨¡å‹è¼‰å…¥æˆåŠŸ")
       
       # é€ä¸€è¼‰å…¥ encoders
       encoders = {}
       for col in cat_features:
           encoder_file = f'encoder_{col}.pkl'
           try:
               if os.path.exists(encoder_file):
                   file_size = os.path.getsize(encoder_file)
                   st.info(f"è¼‰å…¥ {encoder_file} (å¤§å°: {file_size} bytes)")
                   
                   if file_size < 500:  # æª”æ¡ˆå¤ªå°å¯èƒ½æœ‰å•é¡Œ
                       st.warning(f"âš ï¸ {encoder_file} æª”æ¡ˆå¤§å°ç•°å¸¸: {file_size} bytes")
                       
                       # æª¢æŸ¥æ˜¯å¦ç‚º LFS æŒ‡é‡æª”æ¡ˆ
                       try:
                           with open(encoder_file, 'r', encoding='utf-8') as f:
                               content = f.read()
                               st.error(f"æª”æ¡ˆå…§å®¹ï¼ˆLFS æŒ‡é‡ï¼‰: {content}")
                               return None, None, None
                       except:
                           with open(encoder_file, 'rb') as f:
                               content = f.read()
                               st.error(f"æª”æ¡ˆå…§å®¹ï¼ˆäºŒé€²åˆ¶ï¼‰: {content}")
                               return None, None, None
                   
                   encoder = load(encoder_file)
                   encoders[col] = encoder
                   st.success(f"âœ… {col} encoder è¼‰å…¥æˆåŠŸï¼Œé¡åˆ¥æ•¸: {len(encoder.classes_)}")
               else:
                   st.error(f"âŒ æ‰¾ä¸åˆ° {encoder_file}")
                   return None, None, None
           except Exception as e:
               st.error(f"âŒ è¼‰å…¥ {encoder_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
               return None, None, None
       
       # é€ä¸€è¼‰å…¥ scalers
       scalers = {}
       for col in num_features:
           scaler_file = f'scaler_feature_{col}.pkl'
           try:
               if os.path.exists(scaler_file):
                   file_size = os.path.getsize(scaler_file)
                   st.info(f"è¼‰å…¥ {scaler_file} (å¤§å°: {file_size} bytes)")
                   
                   if file_size < 500:  # æª”æ¡ˆå¤ªå°å¯èƒ½æœ‰å•é¡Œ
                       st.warning(f"âš ï¸ {scaler_file} æª”æ¡ˆå¤§å°ç•°å¸¸: {file_size} bytes")
                       
                       # æª¢æŸ¥æ˜¯å¦ç‚º LFS æŒ‡é‡æª”æ¡ˆ
                       try:
                           with open(scaler_file, 'r', encoding='utf-8') as f:
                               content = f.read()
                               st.error(f"æª”æ¡ˆå…§å®¹ï¼ˆLFS æŒ‡é‡ï¼‰: {content}")
                               return None, None, None
                       except:
                           with open(scaler_file, 'rb') as f:
                               content = f.read()
                               st.error(f"æª”æ¡ˆå…§å®¹ï¼ˆäºŒé€²åˆ¶ï¼‰: {content}")
                               return None, None, None
                   
                   scaler = load(scaler_file)
                   scalers[col] = scaler
                   st.success(f"âœ… {col} scaler è¼‰å…¥æˆåŠŸ")
               else:
                   st.error(f"âŒ æ‰¾ä¸åˆ° {scaler_file}")
                   return None, None, None
           except Exception as e:
               st.error(f"âŒ è¼‰å…¥ {scaler_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
               return None, None, None
               
       return model, encoders, scalers
       
   except Exception as e:
       st.error(f"è¼‰å…¥éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
       return None, None, None

# 1. å®šç¾©ç­‰ç´šæ˜ å°„å‡½å¼
def get_level(action_group: str) -> float:
    # æª¢æŸ¥è¼¸å…¥æ˜¯å¦ç‚ºæœ‰æ•ˆå€¼
    if action_group is None or pd.isna(action_group) or action_group == '':
        return -1
        
    # è½‰æ›ç‚ºå­—ç¬¦ä¸²ä»¥ç¢ºä¿ä¸€è‡´æ€§
    action_group = str(action_group).strip()
    
    # ç­‰ç´š 0
    if action_group in {"å…¶ä»–", "ä¿éšªè¦–åœ–ã€ä¿å–®æ˜ç´°ã€è³‡ç”¢ç¸½è¦½ã€ä¿éšªæ˜ç´°"}:
        return 0
    # ç­‰ç´š 1
    if action_group == "æ‰¾æœå‹™ï¼ˆå°‹æ±‚æœå‹™èˆ‡å®¢æœï¼‰":
        return 1
    # ç­‰ç´š 2
    if "å•†å“è³‡è¨Šé " in action_group or action_group == "å¥½åº·å„ªæƒ ":
        return 2
    # ç­‰ç´š 3
    combo3 = {
        "è‡ªç”±é…ï¼æŠ•è³‡è¦åŠƒ", "è‡ªç”±é…", "è¨‚è£½ä¿éšªçµ„åˆ", "è‡ªç”±é…ï¼å¥—é¤"
    }
    if action_group in combo3 or action_group.startswith("è©¦ç®—"):
        return 3
    # ç­‰ç´š 4
    result4 = {
        "è‡ªç”±é…ï¼ä¿éšœè¦åŠƒè©¦ç®—çµæœ",
        "è¨‚è£½ä¿éšªçµ„åˆï¼äººèº«è¦åŠƒè©¦ç®—çµæœ",
        "æˆ‘çš„ä¿éšªè©¦ç®—çµæœ",
        "è¨‚è£½ä¿éšªçµ„åˆï¼æŠ•è³‡è¦åŠƒè©¦ç®—çµæœ",
        "è‡ªç”±é…ï¼é…ç½®æˆ‘çš„åŸºé‡‘è©¦ç®—çµæœ"
    }
    if action_group in result4:
        return 4
    # ç­‰ç´š 5
    if action_group in {
        "ä¿å­˜èˆ‡åˆ†äº«è©¦ç®—çµæœ",
        "ä¿å­˜èˆ‡åˆ†äº«è‡ªç”±é…ã€è¨‚è£½çµ„åˆçµæœ",
        "Lineåˆ†äº«è½‰å‚³"
    }:
        return 5
    # ç­‰ç´š 6
    if action_group in {
        "æŒ‘é¸é ç´„é¡§å•",
        "é ç´„é¡§å•èˆ‡å•†å“è«®è©¢",
        "ç«‹å³æŠ•ä¿"
    }:
        return 6
    # ç­‰ç´š 7.x
    if action_group in {"æ–¹æ¡ˆç¢ºèª", "å¡«å¯«é ç´„è³‡æ–™"}:
        return 7.1
    if action_group in {"è³‡æ–™å¡«å¯«èˆ‡ç¢ºèª", "æŠ•ä¿è³‡æ ¼ç¢ºèª", "æ‰‹æ©Ÿé©—è­‰ç¢¼"}:
        return 7.2
    if action_group == "ç·šä¸Šç¹³è²»":
        return 7.3
    # ç­‰ç´š 8
    if action_group in {"å®Œæˆç¶²è·¯æŠ•ä¿", "å®ŒæˆO2O"}:
        return 8
    # æœªçŸ¥
    return -1

# 2. æ¨è–¦ç­–ç•¥å‡½å¼ï¼ˆä½¿ç”¨ next_action_group åƒæ•¸ï¼‰
def recommend_strategy(
        next_action_group: str,
        history: list[str],
        last_event_time: datetime
    ) -> str | None:
    # æª¢æŸ¥ next_action_group æ˜¯å¦ç‚º None æˆ–ç©ºå€¼
    if next_action_group is None or pd.isna(next_action_group) or next_action_group == '':
        return None
        
    # è™•ç†æ™‚å€å•é¡Œï¼šçµ±ä¸€è½‰æ›ç‚º naive datetime
    now = datetime.now()
    if hasattr(last_event_time, 'tz_localize'):
        # å¦‚æœæ˜¯ pandas Timestamp ä¸”æœ‰æ™‚å€ä¿¡æ¯ï¼Œè½‰æ›ç‚º naive
        if last_event_time.tz is not None:
            last_event_time = last_event_time.tz_localize(None)
    elif hasattr(last_event_time, 'tzinfo'):
        # å¦‚æœæ˜¯ datetime ä¸”æœ‰æ™‚å€ä¿¡æ¯ï¼Œè½‰æ›ç‚º naive
        if last_event_time.tzinfo is not None:
            last_event_time = last_event_time.replace(tzinfo=None)

    # ç­‰ç´š 0
    if next_action_group in {"å…¶ä»–", "ä¿éšªè¦–åœ–ã€ä¿å–®æ˜ç´°ã€è³‡ç”¢ç¸½è¦½ã€ä¿éšªæ˜ç´°"}:
        return None

    # ç­‰ç´š 1ï¼šæ‰¾æœå‹™
    if next_action_group == "æ‰¾æœå‹™ï¼ˆå°‹æ±‚æœå‹™èˆ‡å®¢æœï¼‰":
        s = "ç®­é ­æé†’ã€Œæ‰¾æœå‹™ã€ä»‹é¢åœ¨ç•«é¢ä¸Šæ–¹é¸å–®"
        if history[-10:].count("æ‰¾æœå‹™ï¼ˆå°‹æ±‚æœå‹™èˆ‡å®¢æœï¼‰") > 3:
            s += "ï¼›è®“å³ä¸‹è§’çš„é˜¿ç™¼çš„ã€ŒHi éœ€è¦å¹«å¿™å—ã€ç§€å‡ºä¾†"
        return s

    # ç­‰ç´š 2ï¼šå•†å“è³‡è¨Šé 
    if "å•†å“è³‡è¨Šé " in next_action_group:
        cnt = sum(1 for ag in history[-10:] if ag and "å•†å“è³‡è¨Šé " in ag)
        if cnt >= 3:
            return "é¡¯ç¤ºå…¥å£æ¨å‹•è©¦ç®—"
        return None

    # ç­‰ç´š 2ï¼šå¥½åº·å„ªæƒ 
    if next_action_group == "å¥½åº·å„ªæƒ ":
        if now - last_event_time > timedelta(minutes=3):
            return "å½ˆçª—é¡¯ç¤ºå¥½åº·å„ªæƒ ç›¸é—œè³‡è¨Š"
        return None

    # ç­‰ç´š 3ï¼šæŒ‘é¸çµ„åˆ
    combo3 = {"è‡ªç”±é…ï¼æŠ•è³‡è¦åŠƒ", "è‡ªç”±é…", "è¨‚è£½ä¿éšªçµ„åˆ", "è‡ªç”±é…ï¼å¥—é¤"}
    if next_action_group in combo3:
        prompt = (
            "ä¸çŸ¥é“å¦‚ä½•é–‹å§‹å—ï¼Ÿç°¡å–®ä¸‰å€‹å•é¡Œï¼Œè®“ç³»çµ±æä¾›æœ€é©åˆä½ çš„å•†å“ï¼"
            if next_action_group == "è¨‚è£½ä¿éšªçµ„åˆ"
            else "ä¸€éµæ­é…å€‹äººåŒ–çš„å•†å“çµ„åˆï¼Œåªè¦ 2 åˆ†é˜ï¼"
        )
        return f"å½ˆçª—ï¼šã€Œ{prompt}ã€"

    # ç­‰ç´š 3ï¼šè©¦ç®—
    if next_action_group.startswith("è©¦ç®—"):
        return "å½ˆçª—ï¼šã€Œä¸€éµå¸¶ä½ å®Œæˆè©¦ç®—ï¼Œåªè¦ 2 æ­¥é©Ÿï¼Œå–å¾—å•†å“è²»ç”¨ä¼°ç®—ã€"

    # ç­‰ç´š 4ï¼šè©¦ç®—çµæœ
    result4 = {
        "è‡ªç”±é…ï¼ä¿éšœè¦åŠƒè©¦ç®—çµæœ",
        "è¨‚è£½ä¿éšªçµ„åˆï¼äººèº«è¦åŠƒè©¦ç®—çµæœ",
        "æˆ‘çš„ä¿éšªè©¦ç®—çµæœ",
        "è¨‚è£½ä¿éšªçµ„åˆï¼æŠ•è³‡è¦åŠƒè©¦ç®—çµæœ",
        "è‡ªç”±é…ï¼é…ç½®æˆ‘çš„åŸºé‡‘è©¦ç®—çµæœ"
    }
    if next_action_group in result4:
        return "æä¾›é€²åº¦æé†’ï¼šå°±å¿«å®Œæˆäº†ï¼è©¦ç®—çµæœå³å°‡ç”¢ç”Ÿ"

    # ç­‰ç´š 5ï¼šä¿å­˜ï¼åˆ†äº«çµæœ
    if next_action_group in {
        "ä¿å­˜èˆ‡åˆ†äº«è©¦ç®—çµæœ",
        "ä¿å­˜èˆ‡åˆ†äº«è‡ªç”±é…ã€è¨‚è£½çµ„åˆçµæœ"
    }:
        return "è«®è©¢æŒ‰éˆ•æç¤ºï¼šã€Œå°è©¦ç®—çµæœæœ‰ç–‘å•å—ï¼Ÿé ç´„å…è²»å°ˆäººè§£è®€ï¼ã€"
    if next_action_group == "Lineåˆ†äº«è½‰å‚³":
        return "å½ˆçª—é¼“å‹µæ¨è–¦ã€æ¨æ’­æ¨è–¦çå‹µæ©Ÿåˆ¶æˆ–åˆ†äº«å›é¥‹æ´»å‹•"

    # ç­‰ç´š 6ï¼šæŒ‘é¡§å• / è«®è©¢éœ€æ±‚
    if next_action_group in {"æŒ‘é¸é ç´„é¡§å•", "é ç´„é¡§å•èˆ‡å•†å“è«®è©¢"}:
        return "å½ˆçª—æ¨è–¦é¡§å•ï¼šã€Œé€™ä½å°ˆå®¶æœ€æ“…é•·XXéšªï¼Œ3 åˆ†é˜å…§ç¢ºèªé ç´„ã€"
    if next_action_group == "ç«‹å³æŠ•ä¿":
        return "ç«‹å³æŠ•ä¿æŒ‰éˆ•æˆ–æ©«å¹…CTAï¼šã€Œç«‹å³æŠ•ä¿äº«å„ªæƒ ï¼ã€"

    # ç­‰ç´š 7.1ï¼šæ–¹æ¡ˆç¢ºèª / å¡«å¯«é ç´„è³‡æ–™
    if next_action_group in {"æ–¹æ¡ˆç¢ºèª", "å¡«å¯«é ç´„è³‡æ–™"}:
        if now - last_event_time <= timedelta(minutes=5):
            return (
                "é€²åº¦æé†’ï¼šã€Œå†ä¸‰æ­¥å³å¯å®ŒæˆæŠ•ä¿ã€"
                if next_action_group == "æ–¹æ¡ˆç¢ºèª"
                else "é€²åº¦æé†’ï¼šã€Œå†å…©æ­¥å³å¯å®Œæˆé ç´„ã€"
            )

    # ç­‰ç´š 7.2ï¼šè³‡æ–™å¡«å¯«èˆ‡ç¢ºèª / æŠ•ä¿è³‡æ ¼ç¢ºèª / æ‰‹æ©Ÿé©—è­‰ç¢¼
    if next_action_group in {"è³‡æ–™å¡«å¯«èˆ‡ç¢ºèª", "æŠ•ä¿è³‡æ ¼ç¢ºèª", "æ‰‹æ©Ÿé©—è­‰ç¢¼"}:
        if now - last_event_time > timedelta(minutes=30):
            return "æ©Ÿå™¨äººå¼•å°ï¼šã€Œä¸Šæ¬¡é‚„æ²’å¡«å®Œï¼Ÿé»æ­¤ä¸€éµå›åˆ°æµç¨‹ã€"
        if now - last_event_time <= timedelta(minutes=5):
            return (
                "é€²åº¦æé†’ï¼šã€Œé‚„å·®æœ€å¾Œä¸€æ­¥å°±OKï¼Œå³å°‡å®Œæˆé ç´„ã€"
                if next_action_group == "æ‰‹æ©Ÿé©—è­‰ç¢¼"
                else "é€²åº¦æé†’ï¼šã€Œé‚„å·®æœ€å¾Œå…©æ­¥å°±OKï¼Œå³å°‡å®ŒæˆæŠ•ä¿ã€"
            )

    # ç­‰ç´š 7.3ï¼šç·šä¸Šç¹³è²»
    if next_action_group == "ç·šä¸Šç¹³è²»":
        if now - last_event_time > timedelta(minutes=30):
            return "æ©Ÿå™¨äººå¼•å°ï¼šã€Œä¸Šæ¬¡é‚„æ²’å¡«å®Œï¼Ÿé»æ­¤ä¸€éµå›åˆ°æµç¨‹ã€"
        if now - last_event_time <= timedelta(minutes=5):
            return "é€²åº¦æé†’ï¼šã€Œé‚„å·®æœ€å¾Œä¸€æ­¥å°±OKï¼Œå³å°‡å®ŒæˆæŠ•ä¿ã€"

    # ç­‰ç´š 8ï¼šå®Œæˆç¶²æŠ• / å®ŒæˆO2O
    if next_action_group == "å®Œæˆç¶²è·¯æŠ•ä¿":
        if now - last_event_time > timedelta(minutes=30):
            return (
                "å¯„ç™¼EDMæé†’å³å°‡å®ŒæˆæŠ•ä¿ï¼Œä¸¦é™„ä¸Šé€£çµã€Œé‚„å·®æœ€å¾Œä¸€æ­¥å°±OKï¼Œ"
                "é»æˆ‘å›åˆ°æµç¨‹ã€"
            )
        return "æ„Ÿè¬é æ¨è–¦ï¼šã€Œæ„Ÿè¬æ‚¨ï¼é‚€è«‹å¥½å‹å®Œæˆç¶²æŠ•é€é›™æ–¹ NT$300å…ƒã€"
    if next_action_group == "å®ŒæˆO2O":
        if now - last_event_time > timedelta(minutes=30):
            return (
                "å¯„ç™¼EDMæé†’å³å°‡å®Œæˆé ç´„ï¼Œä¸¦é™„ä¸Šé€£çµã€Œé‚„å·®æœ€å¾Œä¸€æ­¥å°±OKï¼Œ"
                "é»æˆ‘å›åˆ°æµç¨‹ã€"
            )

    return None

# 3. æ ¹æ“š Top1~Top5 é¸å‡º next_action_group
def pick_next_action_group(row) -> str:
    candidates = []
    for i in range(1, 6):
        ag = row[f'Top{i}_next_action_group']
        conf = row[f'Top{i}_confidence']
        
        # æª¢æŸ¥ action_group æ˜¯å¦ç‚ºæœ‰æ•ˆå€¼
        if ag is None or pd.isna(ag) or ag == '':
            lvl = -1  # çµ¦ç„¡æ•ˆå€¼æœ€ä½ç­‰ç´š
        else:
            lvl = get_level(ag)
            
        candidates.append((ag, conf, lvl))
    
    # å…ˆæ¯”ç­‰ç´šï¼Œå†æ¯”æ©Ÿç‡ï¼Œéæ¿¾æ‰ç„¡æ•ˆå€¼
    valid_candidates = [(ag, conf, lvl) for ag, conf, lvl in candidates if lvl >= 0]
    
    if valid_candidates:
        best = max(valid_candidates, key=lambda x: (x[2], x[1]))
        return best[0]
    else:
        # å¦‚æœæ²’æœ‰æœ‰æ•ˆå€™é¸ï¼Œè¿”å›ç¬¬ä¸€å€‹éç©ºå€¼æˆ–é»˜èªå€¼
        for ag, conf, lvl in candidates:
            if ag is not None and not pd.isna(ag) and ag != '':
                return ag
        return "å…¶ä»–"  # é»˜èªå€¼

def safe_label_transform(encoder, value, default=0):
    """
    å®‰å…¨åœ°é€²è¡Œ label encodingï¼Œç¢ºä¿ä¸æœƒç”¢ç”Ÿè² å€¼
    """
    try:
        # è™•ç† NaN, None, ç©ºå­—ç¬¦ä¸²
        if pd.isna(value) or value is None or value == '' or value == 'nan':
            return default
        
        # è½‰æ›ç‚ºå­—ç¬¦ä¸²ï¼ˆä¿æŒèˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼‰
        str_value = str(value).strip()
        
        # å¦‚æœæ˜¯ç©ºå­—ç¬¦ä¸²ï¼Œè¿”å›é»˜èªå€¼
        if str_value == '' or str_value == 'nan':
            return default
        
        # æª¢æŸ¥æ˜¯å¦åœ¨å·²çŸ¥é¡åˆ¥ä¸­
        if str_value in encoder.classes_:
            result = encoder.transform([str_value])[0]
            # ç¢ºä¿çµæœä¸æ˜¯è² æ•¸
            if result < 0:
                print(f"è­¦å‘Šï¼šç·¨ç¢¼çµæœç‚ºè² æ•¸ {result}ï¼Œä½¿ç”¨é»˜èªå€¼ {default}")
                return default
            return result
        else:
            # æœªçŸ¥é¡åˆ¥ï¼Œè¿”å›é»˜èªå€¼
            print(f"è­¦å‘Šï¼šæœªçŸ¥é¡åˆ¥ '{str_value}' (åŸå€¼: {value})ï¼Œä½¿ç”¨é»˜èªå€¼ {default}")
            return default
            
    except Exception as e:
        print(f"Label encoding éŒ¯èª¤ (å€¼: {value}): {e}ï¼Œä½¿ç”¨é»˜èªå€¼ {default}")
        return default

def safe_scale(scaler, value, default=0.0):
    """
    å®‰å…¨åœ°é€²è¡Œæ•¸å€¼æ¨™æº–åŒ–
    """
    try:
        # è™•ç† NaN, None
        if pd.isna(value) or value is None:
            return default
        
        # è½‰æ›ç‚ºæµ®é»æ•¸
        float_value = float(value)
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰é™æ•¸
        if not np.isfinite(float_value):
            return default
        
        # é€²è¡Œæ¨™æº–åŒ–
        scaled_value = scaler.transform([[float_value]])[0][0]
        
        # æª¢æŸ¥çµæœæ˜¯å¦æœ‰æ•ˆ
        if not np.isfinite(scaled_value):
            return default
        
        return scaled_value
        
    except Exception as e:
        print(f"Scaling éŒ¯èª¤ (å€¼: {value}): {e}ï¼Œä½¿ç”¨é»˜èªå€¼ {default}")
        return default

def validate_and_fix_sequence(seq, seq_name):
    """
    é©—è­‰ä¸¦ä¿®å¾©åºåˆ—ä¸­çš„ç„¡æ•ˆå€¼
    """
    seq = np.array(seq)
    
    # æª¢æŸ¥ä¸¦ä¿®å¾©è² å€¼
    negative_mask = seq < 0
    if np.any(negative_mask):
        print(f"è­¦å‘Šï¼š{seq_name} ä¸­ç™¼ç¾ {np.sum(negative_mask)} å€‹è² å€¼ï¼Œå·²ä¿®å¾©ç‚º 0")
        seq[negative_mask] = 0
    
    # æª¢æŸ¥ä¸¦ä¿®å¾© NaN å€¼
    nan_mask = ~np.isfinite(seq)
    if np.any(nan_mask):
        print(f"è­¦å‘Šï¼š{seq_name} ä¸­ç™¼ç¾ {np.sum(nan_mask)} å€‹ç„¡æ•ˆå€¼ï¼Œå·²ä¿®å¾©ç‚º 0")
        seq[nan_mask] = 0
    
    return seq

def predict_from_uploaded_csv(df):
    if model is None or encoders is None or scalers is None:
        st.error("æ¨¡å‹æœªæ­£ç¢ºè¼‰å…¥ï¼Œç„¡æ³•é€²è¡Œé æ¸¬")
        return pd.DataFrame()
    
    # æª¢æŸ¥å¿…è¦æ¬„ä½
    required_columns = ['user_pseudo_id', 'event_time', 'platform', 'action', 'action_group'] \
                       + cat_features + num_features
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        st.error(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
        return pd.DataFrame()
    
    # è³‡æ–™é è™•ç†
    df = df.copy()
    df['event_time'] = pd.to_datetime(df['event_time'])
    
    # çµ±ä¸€è™•ç†æ™‚å€ï¼šå¦‚æœæœ‰æ™‚å€ä¿¡æ¯ï¼Œè½‰æ›ç‚º naive datetime
    if df['event_time'].dt.tz is not None:
        df['event_time'] = df['event_time'].dt.tz_localize(None)
    
    df = df.sort_values(by=["user_pseudo_id", "event_time"])
    
    results = []
    
    for user_id, group in df.groupby("user_pseudo_id"):
        try:
            # å–æœ€å¾Œ SEQ_LEN æ­¥
            last_steps = group.tail(SEQ_LEN)
            pad_len = SEQ_LEN - len(last_steps)
            
            # --- æ“·å– raw features ---
            last = last_steps.iloc[-1]
            raw_last_event_time   = last['event_time']
            raw_last_platform     = last['platform']
            raw_last_action       = last['action']
            raw_last_action_group = last['action_group']
            
            # å–å‰ 2~9 æ­¥çš„ raw action & action_group
            prev_records = {}
            for i in range(2, 10):
                if len(last_steps) >= i:
                    rec = last_steps.iloc[-i]
                    prev_records[f"-{i}_action"]        = rec['action']
                    prev_records[f"-{i}_action_group"]  = rec['action_group']
                else:
                    prev_records[f"-{i}_action"]        = None
                    prev_records[f"-{i}_action_group"]  = None
            
            # --- é¡åˆ¥ç‰¹å¾µç·¨ç¢¼ & å¡«å…… ---
            cat_inputs = []
            for col in cat_features:
                raw_vals = last_steps[col].tolist()
                encoded = [ safe_label_transform(encoders[col], v, default=0)
                            for v in raw_vals ]
                padded = [0]*pad_len + encoded
                seq = validate_and_fix_sequence(padded, f"{col}_sequence")
                cat_inputs.append(np.array(seq, dtype=np.int32).reshape(1, SEQ_LEN))
            
            # --- æ•¸å€¼ç‰¹å¾µæ¨™æº–åŒ– & å¡«å…… ---
            num_inputs = []
            for col in num_features:
                raw_vals = last_steps[col].tolist()
                scaled = [ safe_scale(scalers[col], v, default=0.0)
                           for v in raw_vals ]
                padded = [0.0]*pad_len + scaled
                seq = validate_and_fix_sequence(padded, f"{col}_sequence")
                num_inputs.append(np.array(seq, dtype=np.float32).reshape(1, SEQ_LEN))
            
            all_inputs = cat_inputs + num_inputs

            # é€²è¡Œé æ¸¬ä¸¦å– Top5
            y_pred_action_group, y_pred_online, y_pred_o2o = model.predict(all_inputs, verbose=0)
            top5_idx   = y_pred_action_group[0].argsort()[-5:][::-1]
            top5_confs = y_pred_action_group[0][top5_idx]
            inv_ag     = {i: v for i, v in enumerate(encoders['action_group'].classes_)}
            top5_actions = [inv_ag.get(idx, "æœªçŸ¥") for idx in top5_idx]
            
            # æ­£ç¢ºåœ° build ä¸­ä»‹ dictï¼Œæ³¨æ„ for è¿´åœˆå¾Œè¦æœ‰å†’è™Ÿ
            temp = {}
            for i in range(5):
                temp[f"Top{i+1}_next_action_group"] = top5_actions[i]
                temp[f"Top{i+1}_confidence"]       = round(float(top5_confs[i]), 4)
                
            # å†æŠŠé€™å€‹ dict å‚³çµ¦ pick_next_action_group
            next_ag = pick_next_action_group(temp)
            
            # è¨ˆç®—è¡ŒéŠ·ç­–ç•¥
            strategy = recommend_strategy(
                next_ag,
                [ prev_records.get(f"-{i}_action_group") for i in range(1, 10) ],
                raw_last_event_time
            )
            
            # --- çµ„è£çµæœ ---
            result = {
                "user_pseudo_id":      user_id,
                "last_event_time":     raw_last_event_time,
                "last_platform":       raw_last_platform,
                "last_action":         raw_last_action,
                "last_action_group":   raw_last_action_group,
                **prev_records,
                "Online_conversion_prob":  round(float(y_pred_online[0][0]), 4),
                "O2O_reservation_prob":    round(float(y_pred_o2o[0][0]), 4),
                "Marketing_Strategy":      strategy
            }
            
            # æ·»åŠ  Top1~Top5
            for i in range(5):
                result[f"Top{i+1}_next_action_group"] = top5_actions[i]
                result[f"Top{i+1}_confidence"]       = round(float(top5_confs[i]), 4)
            
            results.append(result)
        
        except Exception as e:
            st.error(f"è™•ç†ç”¨æˆ¶ {user_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
    
    return pd.DataFrame(results)

# UI ä»‹é¢
st.title("ğŸ“Š åœ‹æ³°äººå£½ â€“ å¤šå…ƒè¨ªå®¢è¡Œç‚ºé æ¸¬å·¥å…·")

# è¼‰å…¥æ¨¡å‹ä¸¦é¡¯ç¤ºç‹€æ…‹
with st.spinner("æ­£åœ¨è¼‰å…¥æ¨¡å‹..."):
    if TF_AVAILABLE:
        model, encoders, scalers = load_model_and_encoders()
        if model is not None:
            st.success("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        else:
            st.error("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—")
            st.stop()
    else:
        st.error("âš ï¸ TensorFlow ä¸å¯ç”¨")
        st.stop()

st.markdown("è«‹ä¸Šå‚³åŒ…å«ç”¨æˆ¶è¡Œç‚ºæ­·ç¨‹è³‡æ–™çš„ CSV é€²è¡Œæ¨¡å‹é æ¸¬")

# æª”æ¡ˆä¸Šå‚³å™¨
uploaded_file = st.file_uploader(
    "ğŸ“¥ ä¸Šå‚³ CSVï¼ˆéœ€å«æ¬„ä½ï¼šuser_pseudo_id, event_time, action_group, source, medium, platform, staytime, has_shared, revisit_countï¼‰", 
    type=["csv"]
)

if uploaded_file is not None:
    try:
        # æª¢æŸ¥æ˜¯å¦æ˜¯æ–°ä¸Šå‚³çš„æ–‡ä»¶
        current_file_name = uploaded_file.name
        if (st.session_state.uploaded_file_name != current_file_name or 
            st.session_state.prediction_data is None):
            
            # æ–°æ–‡ä»¶æˆ–æœªè™•ç†éï¼Œé–‹å§‹é æ¸¬
            user_df = pd.read_csv(uploaded_file)
            st.success(f"âœ… æˆåŠŸè®€å– {len(user_df)} ç­†è³‡æ–™")
            
            # === è¼¸å…¥ç¯©é¸å€åŸŸ ===
            with st.expander("ğŸ“… è³‡æ–™æœŸé–“ç¯©é¸", expanded=True):
                col1, col2 = st.columns(2)
                
                # ç²å–è³‡æ–™çš„æ™‚é–“ç¯„åœ
                user_df['event_time'] = pd.to_datetime(user_df['event_time'])
                min_date = user_df['event_time'].min().date()
                max_date = user_df['event_time'].max().date()
                
                with col1:
                    start_date = st.date_input(
                        "èµ·å§‹æ—¥æœŸ", 
                        value=min_date,
                        min_value=min_date,
                        max_value=max_date
                    )
                
                with col2:
                    end_date = st.date_input(
                        "æˆªæ­¢æ—¥æœŸ", 
                        value=max_date,
                        min_value=min_date,
                        max_value=max_date
                    )
                
                # å¥—ç”¨æ—¥æœŸç¯©é¸
                if start_date <= end_date:
                    filtered_df = user_df[
                        (user_df['event_time'].dt.date >= start_date) & 
                        (user_df['event_time'].dt.date <= end_date)
                    ]
                    st.info(f"ç¯©é¸å¾Œè³‡æ–™: {len(filtered_df)} ç­† ({start_date} ~ {end_date})")
                else:
                    st.error("èµ·å§‹æ—¥æœŸä¸èƒ½å¤§æ–¼æˆªæ­¢æ—¥æœŸ")
                    filtered_df = user_df
            
            st.dataframe(filtered_df.head(10), use_container_width=True)

            with st.spinner("æ­£åœ¨é€²è¡Œé æ¸¬..."):
                df_result = predict_from_uploaded_csv(filtered_df)
                
            if not df_result.empty:
                st.session_state.prediction_data = df_result
                st.session_state.all_columns = df_result.columns.tolist()
                st.session_state.uploaded_file_name = current_file_name
                st.session_state.processed_data = filtered_df
                st.success("ğŸ‰ é æ¸¬å®Œæˆï¼")
            else:
                st.error("é æ¸¬å¤±æ•—ï¼Œè«‹æª¢æŸ¥è³‡æ–™æ ¼å¼")
        else:
            # ç›¸åŒæ–‡ä»¶ä¸”å·²è™•ç†éï¼Œç›´æ¥é¡¯ç¤ºä¹‹å‰çš„çµæœ
            st.success(f"âœ… æ–‡ä»¶å·²è™•ç†å®Œæˆ ({len(st.session_state.processed_data)} ç­†è³‡æ–™)")
            st.dataframe(st.session_state.processed_data.head(10), use_container_width=True)
            st.success("ğŸ‰ ä½¿ç”¨å¿«å–çš„é æ¸¬çµæœ")

    except Exception as e:
        st.error(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")

# é æ¸¬çµæœå±•ç¤ºèˆ‡ä¸‹è¼‰
if st.session_state.prediction_data is not None:
    df = st.session_state.prediction_data
    
    # === è¼¸å‡ºç¯©é¸å€åŸŸ ===
    st.markdown("### ğŸ¯ çµæœç¯©é¸")
    
    with st.expander("ğŸ” é€²éšç¯©é¸é¸é …", expanded=True):
        col1, col2 = st.columns(2)
        
        with col1:
            # 1ï¸âƒ£ æœ€è¿‘næ­¥å…§è¸©éç‰¹å®šè¡Œç‚ºçš„ç”¨æˆ¶
            st.markdown("**1ï¸âƒ£ æ­·å²è¡Œç‚ºç¯©é¸**")
            history_steps = st.selectbox("æœ€è¿‘å¹¾æ­¥å…§", options=list(range(2, 10)), index=6)  # é è¨­8æ­¥
            
            # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„ action_group
            history_columns = [f"-{i}_action_group" for i in range(2, 10)]
            all_history_actions = set()
            for col in history_columns:
                if col in df.columns:
                    all_history_actions.update(df[col].dropna().unique())
            all_history_actions = sorted([x for x in all_history_actions if pd.notna(x)])
            
            selected_history_actions = st.multiselect(
                "æ›¾ç¶“åŸ·è¡Œçš„è¡Œç‚º",
                options=all_history_actions,
                help="é¸æ“‡ç”¨æˆ¶åœ¨æ­·å²ä¸­æ›¾ç¶“åŸ·è¡Œéçš„è¡Œç‚º"
            )
        
        with col2:
            # 2ï¸âƒ£ ä¸‹ä¸€æ­¥Top Né æ¸¬ç¯©é¸
            st.markdown("**2ï¸âƒ£ é æ¸¬è¡Œç‚ºç¯©é¸**")
            top_n = st.selectbox("æª¢æŸ¥Topå¹¾çš„é æ¸¬", options=[1, 2, 3, 4, 5], index=0)
            
            # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„é æ¸¬ action_group
            prediction_columns = [f"Top{i}_next_action_group" for i in range(1, 6)]
            all_prediction_actions = set()
            for col in prediction_columns:
                if col in df.columns:
                    all_prediction_actions.update(df[col].dropna().unique())
            all_prediction_actions = sorted([x for x in all_prediction_actions if pd.notna(x)])
            
            selected_prediction_actions = st.multiselect(
                "é æ¸¬çš„ä¸‹ä¸€æ­¥è¡Œç‚º",
                options=all_prediction_actions,
                help="é¸æ“‡é æ¸¬çš„ä¸‹ä¸€æ­¥è¡Œç‚º"
            )
        
        # 3ï¸âƒ£ Top1 æ©Ÿç‡é–€æª»
        st.markdown("**3ï¸âƒ£ é æ¸¬ä¿¡å¿ƒé–€æª»**")
        min_confidence = st.number_input(
            "Top1 æœ€ä½æ©Ÿç‡",
            min_value=0.0,
            max_value=1.0,
            value=0.0,
            step=0.01,
            help="è¨­å®š Top1 é æ¸¬çš„æœ€ä½ä¿¡å¿ƒåˆ†æ•¸ï¼ˆ0-1ä¹‹é–“ï¼‰"
        )
        
        if not (0.0 <= min_confidence <= 1.0):
            st.error("âš ï¸ æ©Ÿç‡å€¼å¿…é ˆåœ¨ 0.0 åˆ° 1.0 ä¹‹é–“ï¼Œè«‹é‡æ–°è¼¸å…¥")
            min_confidence = 0.0
    
    # å¥—ç”¨ç¯©é¸é‚è¼¯
    filtered_df = df.copy()
    filter_conditions = []  # è¨˜éŒ„ç¯©é¸æ¢ä»¶
    
    # 1ï¸âƒ£ æ­·å²è¡Œç‚ºç¯©é¸
    if selected_history_actions:
        history_mask = pd.Series([False] * len(filtered_df))
        for _, row in filtered_df.iterrows():
            for step in range(2, history_steps + 2):
                col_name = f"-{step}_action_group"
                if col_name in row and row[col_name] in selected_history_actions:
                    history_mask[row.name] = True
                    break
        filtered_df = filtered_df[history_mask]
        
        # è¨˜éŒ„ç¯©é¸æ¢ä»¶
        actions_text = "ã€".join(selected_history_actions)
        filter_conditions.append(f"æœ€è¿‘{history_steps}æ­¥å…§æ›¾è¸©éï¼š{actions_text}")
    
    # 2ï¸âƒ£ é æ¸¬è¡Œç‚ºç¯©é¸
    if selected_prediction_actions:
        prediction_mask = pd.Series([False] * len(filtered_df))
        for _, row in filtered_df.iterrows():
            for n in range(1, top_n + 1):
                col_name = f"Top{n}_next_action_group"
                if col_name in row and row[col_name] in selected_prediction_actions:
                    prediction_mask[row.name] = True
                    break
        filtered_df = filtered_df[prediction_mask]
        
        # è¨˜éŒ„ç¯©é¸æ¢ä»¶
        actions_text = "ã€".join(selected_prediction_actions)
        filter_conditions.append(f"ä¸‹ä¸€æ­¥çš„Top{top_n}å¯èƒ½ç‚ºï¼š{actions_text}")
    
    # 3ï¸âƒ£ Top1 æ©Ÿç‡é–€æª»ç¯©é¸
    if min_confidence > 0.0:
        filtered_df = filtered_df[filtered_df['Top1_confidence'] >= min_confidence]
        filter_conditions.append(f"Top1çš„æœ€ä½æ©Ÿç‡éœ€å¤§æ–¼ç­‰æ–¼ï¼š{min_confidence}")
    
    # ğŸ“‹ ç¯©é¸æ¢ä»¶ç¸½çµ
    st.markdown("### ğŸ“‹ ç¯©é¸æ¢ä»¶ç¸½çµ")
    
    if filter_conditions:
        # ç²å–æ—¥æœŸç¯©é¸è³‡è¨Š
        if 'start_date' in locals() and 'end_date' in locals():
            date_range = f"åœ¨ {start_date} ~ {end_date} ä¹‹é–“"
        else:
            date_range = "åœ¨æ‰€é¸æ—¥æœŸç¯„åœå…§"
        
        # æ§‹å»ºæ¢ä»¶æè¿°
        conditions_text = f"""
        **æ‚¨çš„ç¯©é¸æ¢ä»¶ç‚ºï¼š**
        
        {date_range}ï¼Œ
        """
        
        for i, condition in enumerate(filter_conditions):
            if i == 0:
                conditions_text += f"{condition}"
            else:
                conditions_text += f"\nä¸” {condition}"
        
        conditions_text += f"""
        
        **ç•¶å‰æ¢ä»¶ä¸‹ï¼Œå…¨æ•¸ç¬¦åˆçš„ç”¨æˆ¶æ•¸é‡å…±æœ‰ï¼š{len(filtered_df)} äºº**
        """
        
        st.markdown(conditions_text)
        
        # å¦‚æœæœ‰ç¯©é¸ä½†ç„¡çµæœ
        if len(filtered_df) == 0:
            st.warning("âš ï¸ æ²’æœ‰ç¬¦åˆæ‰€æœ‰ç¯©é¸æ¢ä»¶çš„ç”¨æˆ¶")
    else:
        # æ²’æœ‰è¨­å®šä»»ä½•ç¯©é¸æ¢ä»¶
        st.info("ğŸ” ç›®å‰æœªè¨­å®šä»»ä½•ç¯©é¸æ¢ä»¶ï¼Œé¡¯ç¤ºæ‰€æœ‰é æ¸¬çµæœ")
        st.markdown(f"**ç”¨æˆ¶ç¸½æ•¸ï¼š{len(filtered_df)} äºº**")
    
    # é¡¯ç¤ºç¯©é¸çµæœ
    if len(filtered_df) > 0:
        with st.expander("ğŸ“Š æŸ¥çœ‹ç¯©é¸çµæœ", expanded=False):
            st.dataframe(filtered_df, use_container_width=True)
    else:
        st.warning("âš ï¸ æ²’æœ‰ç¬¦åˆç¯©é¸æ¢ä»¶çš„è³‡æ–™")

    # 4ï¸âƒ£ ä¸‹è¼‰è¨­å®š
    if len(filtered_df) > 0:
        with st.expander("ğŸ“¥ ä¸‹è¼‰è¨­å®š", expanded=True):
            st.markdown("**é¸æ“‡è¼¸å‡ºæ¬„ä½**")
            
            # æä¾›å¿«é€Ÿé¸é …
            col1, col2, col3 = st.columns(3)
            with col1:
                if st.button("ğŸ”„ å…¨é¸"):
                    st.session_state.selected_columns = st.session_state.all_columns
            with col2:
                if st.button("ğŸ“Š æ ¸å¿ƒæ¬„ä½"):
                    core_columns = [
                        'user_pseudo_id', 'last_event_time', 'last_action_group',
                        'Top1_next_action_group', 'Top1_confidence', 
                        'Online_conversion_prob', 'O2O_reservation_prob', 'Marketing_Strategy'
                    ]
                    st.session_state.selected_columns = [col for col in core_columns if col in st.session_state.all_columns]
            with col3:
                if st.button("ğŸ¯ é æ¸¬æ¬„ä½"):
                    prediction_cols = [col for col in st.session_state.all_columns if 'Top' in col or 'prob' in col]
                    st.session_state.selected_columns = prediction_cols

            # æ¬„ä½å¤šé¸å™¨
            if 'selected_columns' not in st.session_state:
                st.session_state.selected_columns = st.session_state.all_columns
            
            selected_columns = st.multiselect(
                "é¸æ“‡è¦è¼¸å‡ºçš„æ¬„ä½",
                options=st.session_state.all_columns,
                default=st.session_state.selected_columns,
                key="column_selector"
            )

            if selected_columns:
                csv = filtered_df[selected_columns].to_csv(index=False).encode("utf-8-sig")
                
                # æ ¹æ“šç¯©é¸æ¢ä»¶ç”Ÿæˆæª”å
                if filter_conditions:
                    filename = f"prediction_result_filtered_{len(filtered_df)}users.csv"
                else:
                    filename = f"prediction_result_all_{len(filtered_df)}users.csv"
                
                st.download_button(
                    "ğŸ“¥ ä¸‹è¼‰çµæœ CSV", 
                    data=csv, 
                    file_name=filename, 
                    mime="text/csv",
                    help=f"ä¸‹è¼‰ {len(filtered_df)} ä½ç”¨æˆ¶çš„ç¯©é¸çµæœ"
                )
            else:
                st.warning("è«‹é¸æ“‡è‡³å°‘ä¸€å€‹æ¬„ä½")
    else:
        st.error("ğŸ“¥ ç„¡æ³•ä¸‹è¼‰ï¼šæ²’æœ‰ç¬¦åˆæ¢ä»¶çš„è³‡æ–™")

    # è¦–è¦ºåŒ–åœ–è¡¨
    st.markdown("### ğŸ“Š çµ±è¨ˆåœ–è¡¨")

    if len(df) > 0:
        # åœ–è¡¨ 1: Top1 è¡Œç‚ºåˆ†ä½ˆ
        chart_df = df["Top1_next_action_group"].value_counts().reset_index()
        chart_df.columns = ["action_group", "count"]
        fig1 = px.bar(chart_df, x="action_group", y="count", title="Top1 è¡Œç‚ºåˆ†ä½ˆ")
        st.plotly_chart(fig1, use_container_width=True)

        # åœ–è¡¨ 2: ä¿¡å¿ƒåˆ†æ•¸åˆ†ä½ˆ
        fig2 = px.histogram(df, x="Top1_confidence", nbins=20, title="Top1 ä¿¡å¿ƒåˆ†æ•¸åˆ†ä½ˆ")
        st.plotly_chart(fig2, use_container_width=True)

        # åœ–è¡¨ 3: æ•£é»åœ–
        fig3 = px.scatter(df, x="Top1_confidence", y="Online_conversion_prob", title="ä¿¡å¿ƒåˆ†æ•¸ vs ç·šä¸Šè½‰æ›æ©Ÿç‡")
        st.plotly_chart(fig3, use_container_width=True)

        # åœ–è¡¨ 4: è¡ŒéŠ·ç­–ç•¥æ¯”ä¾‹
        strategy_df = df["Marketing_Strategy"].value_counts().reset_index()
        strategy_df.columns = ["strategy", "count"]
        fig4 = px.pie(strategy_df, names="strategy", values="count", title="å»ºè­°è¡ŒéŠ·ç­–ç•¥æ¯”ä¾‹")
        st.plotly_chart(fig4, use_container_width=True)
